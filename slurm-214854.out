compute003
7
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
{'input_dim': 8, 'output_dim': 4, 'depth': 5, 'lamda': -0.1, 'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 1280, 'epochs': 40, 'cuda': True, 'log_interval': 100, 'exp_scheduler_gamma': 1.0, 'beta': True, 'greatest_path_probability': True, 'model_path': './model/trees/sdt_-0.1_id1'}
Epoch: 01 | Batch: 000 | CrossEntropy-loss: 1.40753 | Correct: 224/1280 | Difference: 1.003172934505735
Epoch: 01 | Batch: 100 | CrossEntropy-loss: 1.28173 | Correct: 470/1280 | Difference: 1.0078078625587161
Epoch: 01 | Batch: 200 | CrossEntropy-loss: 1.17017 | Correct: 807/1280 | Difference: 1.0191977919769661
Epoch: 01 | Batch: 300 | CrossEntropy-loss: 1.05522 | Correct: 876/1280 | Difference: 1.0427980804037629
Epoch: 01 | Batch: 400 | CrossEntropy-loss: 0.93186 | Correct: 893/1280 | Difference: 1.0392595169625478
Epoch: 01 | Batch: 500 | CrossEntropy-loss: 0.84905 | Correct: 911/1280 | Difference: 1.0260096251653539
Epoch: 01 | Batch: 600 | CrossEntropy-loss: 0.81714 | Correct: 846/1280 | Difference: 1.0171862803139031
Epoch: 01 | Batch: 700 | CrossEntropy-loss: 0.79795 | Correct: 888/1280 | Difference: 1.0108507812293666
Epoch: 01 | Batch: 800 | CrossEntropy-loss: 0.75383 | Correct: 909/1280 | Difference: 1.0070625410494904
Epoch: 01 | Batch: 900 | CrossEntropy-loss: 0.72396 | Correct: 951/1280 | Difference: 1.0158546834521138
Epoch: 01 | Batch: 1000 | CrossEntropy-loss: 0.77215 | Correct: 960/1280 | Difference: 1.0349094160808032
Epoch: 01 | Batch: 1100 | CrossEntropy-loss: 0.71176 | Correct: 990/1280 | Difference: 1.0559636591347725
Epoch: 01 | Batch: 1200 | CrossEntropy-loss: 0.68228 | Correct: 976/1280 | Difference: 1.0777780664019816
Epoch: 01 | Batch: 1300 | CrossEntropy-loss: 0.67671 | Correct: 987/1280 | Difference: 1.0989853208340927
Epoch: 01 | Batch: 1400 | CrossEntropy-loss: 0.65749 | Correct: 988/1280 | Difference: 1.1148006705166176
Epoch: 01 | Batch: 1500 | CrossEntropy-loss: 0.68164 | Correct: 973/1280 | Difference: 1.128370446796209

Epoch: 01 | Testing Accuracy: 382427/504607 (75.787%) | Historical Best: 75.787% 

Epoch: 02 | Batch: 000 | CrossEntropy-loss: 0.62012 | Correct: 999/1280 | Difference: 1.146867887541515
Epoch: 02 | Batch: 100 | CrossEntropy-loss: 0.64657 | Correct: 966/1280 | Difference: 1.1615162828349364
Epoch: 02 | Batch: 200 | CrossEntropy-loss: 0.66188 | Correct: 953/1280 | Difference: 1.1807666135866883
Epoch: 02 | Batch: 300 | CrossEntropy-loss: 0.63875 | Correct: 985/1280 | Difference: 1.1971387031215333
Epoch: 02 | Batch: 400 | CrossEntropy-loss: 0.62279 | Correct: 1005/1280 | Difference: 1.21673096804934
Epoch: 02 | Batch: 500 | CrossEntropy-loss: 0.66829 | Correct: 973/1280 | Difference: 1.2362361886427278
Epoch: 02 | Batch: 600 | CrossEntropy-loss: 0.67044 | Correct: 960/1280 | Difference: 1.254321244953376
Epoch: 02 | Batch: 700 | CrossEntropy-loss: 0.61665 | Correct: 1007/1280 | Difference: 1.2671877470482173
Epoch: 02 | Batch: 800 | CrossEntropy-loss: 0.63577 | Correct: 977/1280 | Difference: 1.2804616141165106
Epoch: 02 | Batch: 900 | CrossEntropy-loss: 0.65970 | Correct: 969/1280 | Difference: 1.28873911315243
Epoch: 02 | Batch: 1000 | CrossEntropy-loss: 0.66885 | Correct: 971/1280 | Difference: 1.2896537995273412
Epoch: 02 | Batch: 1100 | CrossEntropy-loss: 0.58869 | Correct: 1014/1280 | Difference: 1.2897620094201236
Epoch: 02 | Batch: 1200 | CrossEntropy-loss: 0.60826 | Correct: 1024/1280 | Difference: 1.290815285169155
Epoch: 02 | Batch: 1300 | CrossEntropy-loss: 0.63155 | Correct: 998/1280 | Difference: 1.2892068870467945
Epoch: 02 | Batch: 1400 | CrossEntropy-loss: 0.63269 | Correct: 992/1280 | Difference: 1.2889509760309943
Epoch: 02 | Batch: 1500 | CrossEntropy-loss: 0.63646 | Correct: 1001/1280 | Difference: 1.2897400465936997

Epoch: 02 | Testing Accuracy: 397675/504607 (78.809%) | Historical Best: 78.809% 

Epoch: 03 | Batch: 000 | CrossEntropy-loss: 0.58629 | Correct: 1020/1280 | Difference: 1.2905338769125567
Epoch: 03 | Batch: 100 | CrossEntropy-loss: 0.60552 | Correct: 1021/1280 | Difference: 1.2913717471833366
Epoch: 03 | Batch: 200 | CrossEntropy-loss: 0.61669 | Correct: 1009/1280 | Difference: 1.2927641488105115
Epoch: 03 | Batch: 300 | CrossEntropy-loss: 0.60821 | Correct: 1011/1280 | Difference: 1.2931851563310908
Epoch: 03 | Batch: 400 | CrossEntropy-loss: 0.63150 | Correct: 988/1280 | Difference: 1.294042419578365
Epoch: 03 | Batch: 500 | CrossEntropy-loss: 0.59185 | Correct: 1013/1280 | Difference: 1.2956092525911924
Epoch: 03 | Batch: 600 | CrossEntropy-loss: 0.62667 | Correct: 1013/1280 | Difference: 1.2964852603888857
Epoch: 03 | Batch: 700 | CrossEntropy-loss: 0.65684 | Correct: 997/1280 | Difference: 1.2976987096400907
Epoch: 03 | Batch: 800 | CrossEntropy-loss: 0.59455 | Correct: 1000/1280 | Difference: 1.2987110550855834
Epoch: 03 | Batch: 900 | CrossEntropy-loss: 0.61253 | Correct: 1012/1280 | Difference: 1.3000531771267831
Epoch: 03 | Batch: 1000 | CrossEntropy-loss: 0.59736 | Correct: 1015/1280 | Difference: 1.301286534426398
Epoch: 03 | Batch: 1100 | CrossEntropy-loss: 0.59289 | Correct: 1009/1280 | Difference: 1.3021292796507424
Epoch: 03 | Batch: 1200 | CrossEntropy-loss: 0.63001 | Correct: 989/1280 | Difference: 1.3028892197430186
Epoch: 03 | Batch: 1300 | CrossEntropy-loss: 0.61138 | Correct: 1002/1280 | Difference: 1.3033923424198914
Epoch: 03 | Batch: 1400 | CrossEntropy-loss: 0.60539 | Correct: 1023/1280 | Difference: 1.3039541113262327
Epoch: 03 | Batch: 1500 | CrossEntropy-loss: 0.57116 | Correct: 1030/1280 | Difference: 1.3039924310751867

Epoch: 03 | Testing Accuracy: 400871/504607 (79.442%) | Historical Best: 79.442% 

Epoch: 04 | Batch: 000 | CrossEntropy-loss: 0.57918 | Correct: 1005/1280 | Difference: 1.3042939666563502
Epoch: 04 | Batch: 100 | CrossEntropy-loss: 0.60386 | Correct: 1012/1280 | Difference: 1.3049328771210331
Epoch: 04 | Batch: 200 | CrossEntropy-loss: 0.57716 | Correct: 1028/1280 | Difference: 1.3051367063609367
Epoch: 04 | Batch: 300 | CrossEntropy-loss: 0.62570 | Correct: 998/1280 | Difference: 1.3055417721738884
Epoch: 04 | Batch: 400 | CrossEntropy-loss: 0.55868 | Correct: 1019/1280 | Difference: 1.3060089949136204
Epoch: 04 | Batch: 500 | CrossEntropy-loss: 0.60686 | Correct: 1015/1280 | Difference: 1.3065551769687669
Epoch: 04 | Batch: 600 | CrossEntropy-loss: 0.57511 | Correct: 1020/1280 | Difference: 1.30709485100173
Epoch: 04 | Batch: 700 | CrossEntropy-loss: 0.57993 | Correct: 1018/1280 | Difference: 1.3088095786723284
Epoch: 04 | Batch: 800 | CrossEntropy-loss: 0.60409 | Correct: 1008/1280 | Difference: 1.3090466339769355
Epoch: 04 | Batch: 900 | CrossEntropy-loss: 0.59924 | Correct: 1007/1280 | Difference: 1.3098239836973484
Epoch: 04 | Batch: 1000 | CrossEntropy-loss: 0.56151 | Correct: 1033/1280 | Difference: 1.3098212280790849
Epoch: 04 | Batch: 1100 | CrossEntropy-loss: 0.57424 | Correct: 1031/1280 | Difference: 1.3095124211049143
Epoch: 04 | Batch: 1200 | CrossEntropy-loss: 0.59727 | Correct: 1013/1280 | Difference: 1.3096236838952509
Epoch: 04 | Batch: 1300 | CrossEntropy-loss: 0.58206 | Correct: 1035/1280 | Difference: 1.3098083815506825
Epoch: 04 | Batch: 1400 | CrossEntropy-loss: 0.56462 | Correct: 1037/1280 | Difference: 1.309873770748261
Epoch: 04 | Batch: 1500 | CrossEntropy-loss: 0.54452 | Correct: 1051/1280 | Difference: 1.309920280768562

Epoch: 04 | Testing Accuracy: 406578/504607 (80.573%) | Historical Best: 80.573% 

Epoch: 05 | Batch: 000 | CrossEntropy-loss: 0.56613 | Correct: 1022/1280 | Difference: 1.309970116868491
Epoch: 05 | Batch: 100 | CrossEntropy-loss: 0.56914 | Correct: 1014/1280 | Difference: 1.3098298230659253
Epoch: 05 | Batch: 200 | CrossEntropy-loss: 0.62228 | Correct: 1018/1280 | Difference: 1.310432189759122
Epoch: 05 | Batch: 300 | CrossEntropy-loss: 0.61911 | Correct: 1022/1280 | Difference: 1.3109726517295928
Epoch: 05 | Batch: 400 | CrossEntropy-loss: 0.57114 | Correct: 1033/1280 | Difference: 1.3105035378207077
Epoch: 05 | Batch: 500 | CrossEntropy-loss: 0.58295 | Correct: 1034/1280 | Difference: 1.3104633048180359
Epoch: 05 | Batch: 600 | CrossEntropy-loss: 0.59330 | Correct: 1019/1280 | Difference: 1.3098874541913792
Epoch: 05 | Batch: 700 | CrossEntropy-loss: 0.58092 | Correct: 1020/1280 | Difference: 1.3102048413230756
Epoch: 05 | Batch: 800 | CrossEntropy-loss: 0.58479 | Correct: 1038/1280 | Difference: 1.3100969935061189
Epoch: 05 | Batch: 900 | CrossEntropy-loss: 0.55775 | Correct: 1051/1280 | Difference: 1.3106567398117197
Epoch: 05 | Batch: 1000 | CrossEntropy-loss: 0.56385 | Correct: 1029/1280 | Difference: 1.3108600363121092
Epoch: 05 | Batch: 1100 | CrossEntropy-loss: 0.63139 | Correct: 1012/1280 | Difference: 1.3113532257522482
Traceback (most recent call last):
  File "sdt_train.py", line 151, in <module>
    train_tree(tree)
  File "sdt_train.py", line 73, in train_tree
    prediction, output, penalty, weights = tree.forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 62, in forward
    _mu, _penalty, _alpha = self._forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 114, in _forward
    return mu, _penalty, torch.mean(torch.stack(half_alpha_list)).detach().cpu().numpy()   # mu contains the path probability for each leaf       
RuntimeError: expected a non-empty list of Tensors
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
{'input_dim': 8, 'output_dim': 4, 'depth': 5, 'lamda': -0.1, 'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 1280, 'epochs': 40, 'cuda': True, 'log_interval': 100, 'exp_scheduler_gamma': 1.0, 'beta': True, 'greatest_path_probability': True, 'model_path': './model/trees/sdt_-0.1_id2'}
Epoch: 01 | Batch: 000 | CrossEntropy-loss: 1.47799 | Correct: 180/1280 | Difference: 0.9764303938468606
Epoch: 01 | Batch: 100 | CrossEntropy-loss: 1.33909 | Correct: 517/1280 | Difference: 1.0392566134645902
Epoch: 01 | Batch: 200 | CrossEntropy-loss: 1.23425 | Correct: 651/1280 | Difference: 1.055696793062398
Epoch: 01 | Batch: 300 | CrossEntropy-loss: 1.13182 | Correct: 646/1280 | Difference: 1.0564197935508737
Epoch: 01 | Batch: 400 | CrossEntropy-loss: 1.03741 | Correct: 651/1280 | Difference: 1.0371116551024793
Epoch: 01 | Batch: 500 | CrossEntropy-loss: 0.93895 | Correct: 659/1280 | Difference: 1.024107496096346
Epoch: 01 | Batch: 600 | CrossEntropy-loss: 0.89100 | Correct: 690/1280 | Difference: 1.0195493346992202
Epoch: 01 | Batch: 700 | CrossEntropy-loss: 0.84450 | Correct: 781/1280 | Difference: 1.031827919016754
Epoch: 01 | Batch: 800 | CrossEntropy-loss: 0.81088 | Correct: 937/1280 | Difference: 1.0437257707449352
Epoch: 01 | Batch: 900 | CrossEntropy-loss: 0.77281 | Correct: 938/1280 | Difference: 1.0446203463385324
Epoch: 01 | Batch: 1000 | CrossEntropy-loss: 0.71678 | Correct: 957/1280 | Difference: 1.04863619238868
Epoch: 01 | Batch: 1100 | CrossEntropy-loss: 0.75269 | Correct: 930/1280 | Difference: 1.056922822496499
Epoch: 01 | Batch: 1200 | CrossEntropy-loss: 0.69658 | Correct: 974/1280 | Difference: 1.0727811703681447
Epoch: 01 | Batch: 1300 | CrossEntropy-loss: 0.70366 | Correct: 959/1280 | Difference: 1.0877628358361866
Epoch: 01 | Batch: 1400 | CrossEntropy-loss: 0.66502 | Correct: 981/1280 | Difference: 1.1048679291090437
Epoch: 01 | Batch: 1500 | CrossEntropy-loss: 0.69459 | Correct: 963/1280 | Difference: 1.1247006789885003

Epoch: 01 | Testing Accuracy: 377897/504607 (74.889%) | Historical Best: 74.889% 

Epoch: 02 | Batch: 000 | CrossEntropy-loss: 0.67970 | Correct: 965/1280 | Difference: 1.1413196565221142
Epoch: 02 | Batch: 100 | CrossEntropy-loss: 0.65799 | Correct: 977/1280 | Difference: 1.1609002582793215
Epoch: 02 | Batch: 200 | CrossEntropy-loss: 0.64431 | Correct: 971/1280 | Difference: 1.1847510654263294
Epoch: 02 | Batch: 300 | CrossEntropy-loss: 0.68162 | Correct: 957/1280 | Difference: 1.2064208164383006
Epoch: 02 | Batch: 400 | CrossEntropy-loss: 0.65046 | Correct: 960/1280 | Difference: 1.2237869984123377
Epoch: 02 | Batch: 500 | CrossEntropy-loss: 0.58855 | Correct: 999/1280 | Difference: 1.2419448589795943
Epoch: 02 | Batch: 600 | CrossEntropy-loss: 0.61275 | Correct: 969/1280 | Difference: 1.2584412057706318
Epoch: 02 | Batch: 700 | CrossEntropy-loss: 0.61978 | Correct: 974/1280 | Difference: 1.273107273015826
Epoch: 02 | Batch: 800 | CrossEntropy-loss: 0.63436 | Correct: 958/1280 | Difference: 1.284595721444579
Epoch: 02 | Batch: 900 | CrossEntropy-loss: 0.61401 | Correct: 979/1280 | Difference: 1.295307135060766
Epoch: 02 | Batch: 1000 | CrossEntropy-loss: 0.61572 | Correct: 971/1280 | Difference: 1.3060064070511672
Epoch: 02 | Batch: 1100 | CrossEntropy-loss: 0.57339 | Correct: 993/1280 | Difference: 1.3119029281988404
Epoch: 02 | Batch: 1200 | CrossEntropy-loss: 0.61264 | Correct: 994/1280 | Difference: 1.3157119263480461
Epoch: 02 | Batch: 1300 | CrossEntropy-loss: 0.61056 | Correct: 971/1280 | Difference: 1.3168326355737692
Epoch: 02 | Batch: 1400 | CrossEntropy-loss: 0.60047 | Correct: 998/1280 | Difference: 1.317238918699781
Epoch: 02 | Batch: 1500 | CrossEntropy-loss: 0.59733 | Correct: 981/1280 | Difference: 1.31728440128821

Epoch: 02 | Testing Accuracy: 389462/504607 (77.181%) | Historical Best: 77.181% 

Epoch: 03 | Batch: 000 | CrossEntropy-loss: 0.61630 | Correct: 996/1280 | Difference: 1.3172396376836253
Epoch: 03 | Batch: 100 | CrossEntropy-loss: 0.55962 | Correct: 1027/1280 | Difference: 1.317319543934411
Epoch: 03 | Batch: 200 | CrossEntropy-loss: 0.59272 | Correct: 1007/1280 | Difference: 1.3170023998919893
Epoch: 03 | Batch: 300 | CrossEntropy-loss: 0.60322 | Correct: 988/1280 | Difference: 1.3163961476471955
Epoch: 03 | Batch: 400 | CrossEntropy-loss: 0.60431 | Correct: 1011/1280 | Difference: 1.315463672986895
Epoch: 03 | Batch: 500 | CrossEntropy-loss: 0.57672 | Correct: 1001/1280 | Difference: 1.3150387622934054
Epoch: 03 | Batch: 600 | CrossEntropy-loss: 0.60211 | Correct: 1007/1280 | Difference: 1.3152281651044093
Epoch: 03 | Batch: 700 | CrossEntropy-loss: 0.57294 | Correct: 1002/1280 | Difference: 1.3150471515513966
Epoch: 03 | Batch: 800 | CrossEntropy-loss: 0.63281 | Correct: 984/1280 | Difference: 1.315109037932527
Epoch: 03 | Batch: 900 | CrossEntropy-loss: 0.58377 | Correct: 1010/1280 | Difference: 1.315016947974029
Epoch: 03 | Batch: 1000 | CrossEntropy-loss: 0.57577 | Correct: 1001/1280 | Difference: 1.3146900088900701
Epoch: 03 | Batch: 1100 | CrossEntropy-loss: 0.59802 | Correct: 1006/1280 | Difference: 1.314685109501845
Epoch: 03 | Batch: 1200 | CrossEntropy-loss: 0.62652 | Correct: 980/1280 | Difference: 1.3142021756536397
Epoch: 03 | Batch: 1300 | CrossEntropy-loss: 0.59347 | Correct: 1004/1280 | Difference: 1.3143012351882932
Epoch: 03 | Batch: 1400 | CrossEntropy-loss: 0.57718 | Correct: 999/1280 | Difference: 1.3136944799434058
Epoch: 03 | Batch: 1500 | CrossEntropy-loss: 0.61169 | Correct: 984/1280 | Difference: 1.3130702130241878

Epoch: 03 | Testing Accuracy: 393211/504607 (77.924%) | Historical Best: 77.924% 

Epoch: 04 | Batch: 000 | CrossEntropy-loss: 0.58791 | Correct: 1011/1280 | Difference: 1.312948345565386
Epoch: 04 | Batch: 100 | CrossEntropy-loss: 0.62529 | Correct: 984/1280 | Difference: 1.3126509242486695
Epoch: 04 | Batch: 200 | CrossEntropy-loss: 0.58152 | Correct: 996/1280 | Difference: 1.3121824021898856
Epoch: 04 | Batch: 300 | CrossEntropy-loss: 0.55940 | Correct: 1003/1280 | Difference: 1.3118671898356
Epoch: 04 | Batch: 400 | CrossEntropy-loss: 0.63586 | Correct: 967/1280 | Difference: 1.3119318380209186
Epoch: 04 | Batch: 500 | CrossEntropy-loss: 0.59147 | Correct: 1015/1280 | Difference: 1.311420266152701
Epoch: 04 | Batch: 600 | CrossEntropy-loss: 0.60618 | Correct: 986/1280 | Difference: 1.311596270836741
Epoch: 04 | Batch: 700 | CrossEntropy-loss: 0.60816 | Correct: 994/1280 | Difference: 1.3115628102283485
Epoch: 04 | Batch: 800 | CrossEntropy-loss: 0.57548 | Correct: 1014/1280 | Difference: 1.3111768919650455
Epoch: 04 | Batch: 900 | CrossEntropy-loss: 0.56975 | Correct: 1015/1280 | Difference: 1.3104903436472468
Epoch: 04 | Batch: 1000 | CrossEntropy-loss: 0.56053 | Correct: 1004/1280 | Difference: 1.3102073498488411
Epoch: 04 | Batch: 1100 | CrossEntropy-loss: 0.58883 | Correct: 996/1280 | Difference: 1.3103013821852112
Epoch: 04 | Batch: 1200 | CrossEntropy-loss: 0.54057 | Correct: 1025/1280 | Difference: 1.309614554680937
Epoch: 04 | Batch: 1300 | CrossEntropy-loss: 0.60891 | Correct: 989/1280 | Difference: 1.3100321225328655
Epoch: 04 | Batch: 1400 | CrossEntropy-loss: 0.59875 | Correct: 980/1280 | Difference: 1.3094895128517394
Epoch: 04 | Batch: 1500 | CrossEntropy-loss: 0.57848 | Correct: 1014/1280 | Difference: 1.3096106070034799

Epoch: 04 | Testing Accuracy: 393696/504607 (78.020%) | Historical Best: 78.020% 

Epoch: 05 | Batch: 000 | CrossEntropy-loss: 0.56963 | Correct: 1005/1280 | Difference: 1.3094901463451938
Epoch: 05 | Batch: 100 | CrossEntropy-loss: 0.60874 | Correct: 985/1280 | Difference: 1.309185267288292
Epoch: 05 | Batch: 200 | CrossEntropy-loss: 0.60743 | Correct: 981/1280 | Difference: 1.3090810542873892
Epoch: 05 | Batch: 300 | CrossEntropy-loss: 0.59597 | Correct: 991/1280 | Difference: 1.309211869103497
Epoch: 05 | Batch: 400 | CrossEntropy-loss: 0.55923 | Correct: 1031/1280 | Difference: 1.3094514372691364
Epoch: 05 | Batch: 500 | CrossEntropy-loss: 0.62041 | Correct: 986/1280 | Difference: 1.3093427283020354
Epoch: 05 | Batch: 600 | CrossEntropy-loss: 0.53555 | Correct: 1021/1280 | Difference: 1.3105843210144548
Traceback (most recent call last):
  File "sdt_train.py", line 151, in <module>
    train_tree(tree)
  File "sdt_train.py", line 73, in train_tree
    prediction, output, penalty, weights = tree.forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 62, in forward
    _mu, _penalty, _alpha = self._forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 114, in _forward
    return mu, _penalty, torch.mean(torch.stack(half_alpha_list)).detach().cpu().numpy()   # mu contains the path probability for each leaf       
RuntimeError: expected a non-empty list of Tensors
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
{'input_dim': 8, 'output_dim': 4, 'depth': 5, 'lamda': -0.1, 'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 1280, 'epochs': 40, 'cuda': True, 'log_interval': 100, 'exp_scheduler_gamma': 1.0, 'beta': True, 'greatest_path_probability': True, 'model_path': './model/trees/sdt_-0.1_id3'}
Epoch: 01 | Batch: 000 | CrossEntropy-loss: 1.40855 | Correct: 322/1280 | Difference: 0.9584057015772602
Epoch: 01 | Batch: 100 | CrossEntropy-loss: 1.29072 | Correct: 405/1280 | Difference: 0.9369715918312577
Epoch: 01 | Batch: 200 | CrossEntropy-loss: 1.18804 | Correct: 745/1280 | Difference: 0.9269362010016412
Epoch: 01 | Batch: 300 | CrossEntropy-loss: 1.07434 | Correct: 830/1280 | Difference: 0.9457983646896022
Epoch: 01 | Batch: 400 | CrossEntropy-loss: 0.99665 | Correct: 832/1280 | Difference: 0.9391105078125218
Epoch: 01 | Batch: 500 | CrossEntropy-loss: 0.92705 | Correct: 827/1280 | Difference: 0.9422034235347202
Epoch: 01 | Batch: 600 | CrossEntropy-loss: 0.83857 | Correct: 920/1280 | Difference: 0.9465216186159289
Epoch: 01 | Batch: 700 | CrossEntropy-loss: 0.80822 | Correct: 922/1280 | Difference: 0.9556657944966344
Epoch: 01 | Batch: 800 | CrossEntropy-loss: 0.79343 | Correct: 933/1280 | Difference: 0.9790919111987411
Epoch: 01 | Batch: 900 | CrossEntropy-loss: 0.76606 | Correct: 918/1280 | Difference: 1.0018380685438102
Epoch: 01 | Batch: 1000 | CrossEntropy-loss: 0.74742 | Correct: 943/1280 | Difference: 1.0272283313303519
Epoch: 01 | Batch: 1100 | CrossEntropy-loss: 0.69689 | Correct: 946/1280 | Difference: 1.0364507959930278
Epoch: 01 | Batch: 1200 | CrossEntropy-loss: 0.73367 | Correct: 918/1280 | Difference: 1.0511764109264314
Epoch: 01 | Batch: 1300 | CrossEntropy-loss: 0.71931 | Correct: 904/1280 | Difference: 1.0672539726116366
Epoch: 01 | Batch: 1400 | CrossEntropy-loss: 0.63741 | Correct: 969/1280 | Difference: 1.0831142215389036
Epoch: 01 | Batch: 1500 | CrossEntropy-loss: 0.65059 | Correct: 961/1280 | Difference: 1.091085434306227

Epoch: 01 | Testing Accuracy: 372773/504607 (73.874%) | Historical Best: 73.874% 

Epoch: 02 | Batch: 000 | CrossEntropy-loss: 0.67440 | Correct: 943/1280 | Difference: 1.100026600337554
Epoch: 02 | Batch: 100 | CrossEntropy-loss: 0.67920 | Correct: 938/1280 | Difference: 1.1155980433728374
Epoch: 02 | Batch: 200 | CrossEntropy-loss: 0.63984 | Correct: 950/1280 | Difference: 1.1358864898395236
Epoch: 02 | Batch: 300 | CrossEntropy-loss: 0.66364 | Correct: 938/1280 | Difference: 1.1583007352729402
Epoch: 02 | Batch: 400 | CrossEntropy-loss: 0.66000 | Correct: 959/1280 | Difference: 1.1765415811897193
Epoch: 02 | Batch: 500 | CrossEntropy-loss: 0.65490 | Correct: 944/1280 | Difference: 1.1942411722145942
Epoch: 02 | Batch: 600 | CrossEntropy-loss: 0.62280 | Correct: 973/1280 | Difference: 1.212506505552012
Epoch: 02 | Batch: 700 | CrossEntropy-loss: 0.67798 | Correct: 937/1280 | Difference: 1.2303507297048302
Epoch: 02 | Batch: 800 | CrossEntropy-loss: 0.60063 | Correct: 977/1280 | Difference: 1.2550689745173071
Epoch: 02 | Batch: 900 | CrossEntropy-loss: 0.59858 | Correct: 980/1280 | Difference: 1.2734619586238787
Epoch: 02 | Batch: 1000 | CrossEntropy-loss: 0.60118 | Correct: 994/1280 | Difference: 1.2780318065515028
Epoch: 02 | Batch: 1100 | CrossEntropy-loss: 0.59442 | Correct: 967/1280 | Difference: 1.2686462409971802
Epoch: 02 | Batch: 1200 | CrossEntropy-loss: 0.62384 | Correct: 956/1280 | Difference: 1.2607062601450034
Epoch: 02 | Batch: 1300 | CrossEntropy-loss: 0.60920 | Correct: 972/1280 | Difference: 1.258268765344198
Epoch: 02 | Batch: 1400 | CrossEntropy-loss: 0.59741 | Correct: 996/1280 | Difference: 1.2595022857060774
Epoch: 02 | Batch: 1500 | CrossEntropy-loss: 0.61742 | Correct: 967/1280 | Difference: 1.2606297522592036

Epoch: 02 | Testing Accuracy: 379886/504607 (75.284%) | Historical Best: 75.284% 

Epoch: 03 | Batch: 000 | CrossEntropy-loss: 0.61234 | Correct: 973/1280 | Difference: 1.26141236570644
Epoch: 03 | Batch: 100 | CrossEntropy-loss: 0.57101 | Correct: 998/1280 | Difference: 1.2620765970062533
Epoch: 03 | Batch: 200 | CrossEntropy-loss: 0.63140 | Correct: 967/1280 | Difference: 1.263339699352475
Epoch: 03 | Batch: 300 | CrossEntropy-loss: 0.61928 | Correct: 981/1280 | Difference: 1.264990523582232
Epoch: 03 | Batch: 400 | CrossEntropy-loss: 0.60224 | Correct: 997/1280 | Difference: 1.2666257118124928
Epoch: 03 | Batch: 500 | CrossEntropy-loss: 0.59016 | Correct: 981/1280 | Difference: 1.2674373656391413
Epoch: 03 | Batch: 600 | CrossEntropy-loss: 0.64098 | Correct: 975/1280 | Difference: 1.2693533042449812
Epoch: 03 | Batch: 700 | CrossEntropy-loss: 0.62464 | Correct: 969/1280 | Difference: 1.271010725228369
Epoch: 03 | Batch: 800 | CrossEntropy-loss: 0.57961 | Correct: 1029/1280 | Difference: 1.2720979213836925
Epoch: 03 | Batch: 900 | CrossEntropy-loss: 0.61857 | Correct: 990/1280 | Difference: 1.2727396161145539
Epoch: 03 | Batch: 1000 | CrossEntropy-loss: 0.57657 | Correct: 1003/1280 | Difference: 1.2737253364389889
Epoch: 03 | Batch: 1100 | CrossEntropy-loss: 0.59002 | Correct: 999/1280 | Difference: 1.2749214837440537
Epoch: 03 | Batch: 1200 | CrossEntropy-loss: 0.58612 | Correct: 998/1280 | Difference: 1.2749978110249875
Epoch: 03 | Batch: 1300 | CrossEntropy-loss: 0.57123 | Correct: 1007/1280 | Difference: 1.275819977377268
Epoch: 03 | Batch: 1400 | CrossEntropy-loss: 0.56214 | Correct: 1009/1280 | Difference: 1.2757908266265077
Epoch: 03 | Batch: 1500 | CrossEntropy-loss: 0.62107 | Correct: 987/1280 | Difference: 1.2759712890177513

Epoch: 03 | Testing Accuracy: 393559/504607 (77.993%) | Historical Best: 77.993% 

Epoch: 04 | Batch: 000 | CrossEntropy-loss: 0.61798 | Correct: 983/1280 | Difference: 1.2758770541363242
Epoch: 04 | Batch: 100 | CrossEntropy-loss: 0.63887 | Correct: 982/1280 | Difference: 1.2762807107962575
Epoch: 04 | Batch: 200 | CrossEntropy-loss: 0.57463 | Correct: 1014/1280 | Difference: 1.2764827701862282
Epoch: 04 | Batch: 300 | CrossEntropy-loss: 0.63874 | Correct: 966/1280 | Difference: 1.2764426277131922
Epoch: 04 | Batch: 400 | CrossEntropy-loss: 0.56408 | Correct: 1012/1280 | Difference: 1.2766335358569245
Epoch: 04 | Batch: 500 | CrossEntropy-loss: 0.56468 | Correct: 1010/1280 | Difference: 1.2769719169952978
Epoch: 04 | Batch: 600 | CrossEntropy-loss: 0.58611 | Correct: 1007/1280 | Difference: 1.2776630888228284
Epoch: 04 | Batch: 700 | CrossEntropy-loss: 0.60235 | Correct: 982/1280 | Difference: 1.2776856552633418
Epoch: 04 | Batch: 800 | CrossEntropy-loss: 0.61514 | Correct: 979/1280 | Difference: 1.2784541833283438
Epoch: 04 | Batch: 900 | CrossEntropy-loss: 0.56064 | Correct: 1016/1280 | Difference: 1.278954818188548
Epoch: 04 | Batch: 1000 | CrossEntropy-loss: 0.56030 | Correct: 1028/1280 | Difference: 1.279338099427881
Epoch: 04 | Batch: 1100 | CrossEntropy-loss: 0.60586 | Correct: 985/1280 | Difference: 1.279953790760048
Epoch: 04 | Batch: 1200 | CrossEntropy-loss: 0.55712 | Correct: 1018/1280 | Difference: 1.2799080941225693
Epoch: 04 | Batch: 1300 | CrossEntropy-loss: 0.58888 | Correct: 1001/1280 | Difference: 1.2804715564831561
Epoch: 04 | Batch: 1400 | CrossEntropy-loss: 0.60075 | Correct: 994/1280 | Difference: 1.280556560365397
Epoch: 04 | Batch: 1500 | CrossEntropy-loss: 0.61869 | Correct: 978/1280 | Difference: 1.2806644398389677

Epoch: 04 | Testing Accuracy: 394503/504607 (78.180%) | Historical Best: 78.180% 

Epoch: 05 | Batch: 000 | CrossEntropy-loss: 0.58522 | Correct: 1003/1280 | Difference: 1.2806669261361054
Epoch: 05 | Batch: 100 | CrossEntropy-loss: 0.58335 | Correct: 1005/1280 | Difference: 1.280905311453196
Epoch: 05 | Batch: 200 | CrossEntropy-loss: 0.57001 | Correct: 1009/1280 | Difference: 1.2810537988320607
Epoch: 05 | Batch: 300 | CrossEntropy-loss: 0.60153 | Correct: 983/1280 | Difference: 1.2810309505852595
Epoch: 05 | Batch: 400 | CrossEntropy-loss: 0.53888 | Correct: 1030/1280 | Difference: 1.2814272055711642
Epoch: 05 | Batch: 500 | CrossEntropy-loss: 0.60573 | Correct: 974/1280 | Difference: 1.28155322104683
Epoch: 05 | Batch: 600 | CrossEntropy-loss: 0.57038 | Correct: 1023/1280 | Difference: 1.28272667644567
Epoch: 05 | Batch: 700 | CrossEntropy-loss: 0.57450 | Correct: 1014/1280 | Difference: 1.2850170688570808
Traceback (most recent call last):
  File "sdt_train.py", line 151, in <module>
    train_tree(tree)
  File "sdt_train.py", line 73, in train_tree
    prediction, output, penalty, weights = tree.forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 62, in forward
    _mu, _penalty, _alpha = self._forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 114, in _forward
    return mu, _penalty, torch.mean(torch.stack(half_alpha_list)).detach().cpu().numpy()   # mu contains the path probability for each leaf       
RuntimeError: expected a non-empty list of Tensors
