compute003
6
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
{'input_dim': 8, 'output_dim': 4, 'depth': 5, 'lamda': -0.01, 'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 1280, 'epochs': 40, 'cuda': True, 'log_interval': 100, 'exp_scheduler_gamma': 1.0, 'beta': True, 'greatest_path_probability': True, 'model_path': './model/trees/sdt_-0.01_id1'}
Epoch: 01 | Batch: 000 | CrossEntropy-loss: 1.38822 | Correct: 218/1280 | Difference: 0.9762496659224683
Epoch: 01 | Batch: 100 | CrossEntropy-loss: 1.27662 | Correct: 781/1280 | Difference: 0.9605353843975027
Epoch: 01 | Batch: 200 | CrossEntropy-loss: 1.14930 | Correct: 725/1280 | Difference: 0.9718449820416402
Epoch: 01 | Batch: 300 | CrossEntropy-loss: 1.04646 | Correct: 845/1280 | Difference: 0.9601472322001112
Epoch: 01 | Batch: 400 | CrossEntropy-loss: 0.93308 | Correct: 899/1280 | Difference: 0.9400669852815404
Epoch: 01 | Batch: 500 | CrossEntropy-loss: 0.87141 | Correct: 916/1280 | Difference: 0.9269494000642243
Epoch: 01 | Batch: 600 | CrossEntropy-loss: 0.81677 | Correct: 938/1280 | Difference: 0.912015575325994
Epoch: 01 | Batch: 700 | CrossEntropy-loss: 0.75866 | Correct: 955/1280 | Difference: 0.8976658701425502
Epoch: 01 | Batch: 800 | CrossEntropy-loss: 0.72089 | Correct: 942/1280 | Difference: 0.8856307200779113
Epoch: 01 | Batch: 900 | CrossEntropy-loss: 0.71067 | Correct: 953/1280 | Difference: 0.8742591812349197
Epoch: 01 | Batch: 1000 | CrossEntropy-loss: 0.67262 | Correct: 974/1280 | Difference: 0.864404071896773
Epoch: 01 | Batch: 1100 | CrossEntropy-loss: 0.65674 | Correct: 964/1280 | Difference: 0.8591687098560044
Epoch: 01 | Batch: 1200 | CrossEntropy-loss: 0.66576 | Correct: 948/1280 | Difference: 0.858344467695074
Epoch: 01 | Batch: 1300 | CrossEntropy-loss: 0.64349 | Correct: 966/1280 | Difference: 0.8605990629960064
Epoch: 01 | Batch: 1400 | CrossEntropy-loss: 0.61289 | Correct: 967/1280 | Difference: 0.8636021986098698
Epoch: 01 | Batch: 1500 | CrossEntropy-loss: 0.60672 | Correct: 974/1280 | Difference: 0.8672101045573063

Epoch: 01 | Testing Accuracy: 380256/504607 (75.357%) | Historical Best: 75.357% 

Epoch: 02 | Batch: 000 | CrossEntropy-loss: 0.57180 | Correct: 1004/1280 | Difference: 0.8706614277015196
Epoch: 02 | Batch: 100 | CrossEntropy-loss: 0.55594 | Correct: 1010/1280 | Difference: 0.8764391632319011
Epoch: 02 | Batch: 200 | CrossEntropy-loss: 0.60860 | Correct: 998/1280 | Difference: 0.884978537586807
Epoch: 02 | Batch: 300 | CrossEntropy-loss: 0.56274 | Correct: 1037/1280 | Difference: 0.8929684828166087
Epoch: 02 | Batch: 400 | CrossEntropy-loss: 0.54488 | Correct: 1055/1280 | Difference: 0.901760923518831
Epoch: 02 | Batch: 500 | CrossEntropy-loss: 0.57146 | Correct: 1009/1280 | Difference: 0.9104630653603385
Epoch: 02 | Batch: 600 | CrossEntropy-loss: 0.51974 | Correct: 1044/1280 | Difference: 0.92174510915207
Epoch: 02 | Batch: 700 | CrossEntropy-loss: 0.52782 | Correct: 1038/1280 | Difference: 0.9339493826078326
Epoch: 02 | Batch: 800 | CrossEntropy-loss: 0.48698 | Correct: 1056/1280 | Difference: 0.9459720448197703
Epoch: 02 | Batch: 900 | CrossEntropy-loss: 0.51300 | Correct: 1038/1280 | Difference: 0.9583940768161857
Epoch: 02 | Batch: 1000 | CrossEntropy-loss: 0.52878 | Correct: 1026/1280 | Difference: 0.97052707417768
Epoch: 02 | Batch: 1100 | CrossEntropy-loss: 0.55894 | Correct: 1026/1280 | Difference: 0.9832725118850024
Epoch: 02 | Batch: 1200 | CrossEntropy-loss: 0.49292 | Correct: 1046/1280 | Difference: 0.9980798349223289
Epoch: 02 | Batch: 1300 | CrossEntropy-loss: 0.53254 | Correct: 1016/1280 | Difference: 1.008077989418098
Epoch: 02 | Batch: 1400 | CrossEntropy-loss: 0.50008 | Correct: 1041/1280 | Difference: 1.011281634217245
Epoch: 02 | Batch: 1500 | CrossEntropy-loss: 0.47749 | Correct: 1048/1280 | Difference: 1.011945740761789

Epoch: 02 | Testing Accuracy: 407971/504607 (80.849%) | Historical Best: 80.849% 

Epoch: 03 | Batch: 000 | CrossEntropy-loss: 0.48234 | Correct: 1047/1280 | Difference: 1.012214549344935
Epoch: 03 | Batch: 100 | CrossEntropy-loss: 0.47987 | Correct: 1048/1280 | Difference: 1.0158481909250041
Epoch: 03 | Batch: 200 | CrossEntropy-loss: 0.49985 | Correct: 1032/1280 | Difference: 1.0264269905767758
Epoch: 03 | Batch: 300 | CrossEntropy-loss: 0.47426 | Correct: 1047/1280 | Difference: 1.0350782978158548
Epoch: 03 | Batch: 400 | CrossEntropy-loss: 0.47948 | Correct: 1051/1280 | Difference: 1.0418849054691037
Epoch: 03 | Batch: 500 | CrossEntropy-loss: 0.48864 | Correct: 1019/1280 | Difference: 1.0459781924165052
Epoch: 03 | Batch: 600 | CrossEntropy-loss: 0.47244 | Correct: 1052/1280 | Difference: 1.0495221730475761
Epoch: 03 | Batch: 700 | CrossEntropy-loss: 0.50119 | Correct: 1026/1280 | Difference: 1.051072788676692
Epoch: 03 | Batch: 800 | CrossEntropy-loss: 0.46876 | Correct: 1063/1280 | Difference: 1.0503320946098773
Epoch: 03 | Batch: 900 | CrossEntropy-loss: 0.46641 | Correct: 1036/1280 | Difference: 1.0499591107280641
Epoch: 03 | Batch: 1000 | CrossEntropy-loss: 0.49622 | Correct: 1036/1280 | Difference: 1.050910129359729
Epoch: 03 | Batch: 1100 | CrossEntropy-loss: 0.46583 | Correct: 1048/1280 | Difference: 1.0544427408347383
Epoch: 03 | Batch: 1200 | CrossEntropy-loss: 0.45552 | Correct: 1061/1280 | Difference: 1.055467588537545
Epoch: 03 | Batch: 1300 | CrossEntropy-loss: 0.44685 | Correct: 1044/1280 | Difference: 1.053305314943996
Epoch: 03 | Batch: 1400 | CrossEntropy-loss: 0.46716 | Correct: 1051/1280 | Difference: 1.0528347335931252
Epoch: 03 | Batch: 1500 | CrossEntropy-loss: 0.47945 | Correct: 1048/1280 | Difference: 1.0536651885421293

Epoch: 03 | Testing Accuracy: 411199/504607 (81.489%) | Historical Best: 81.489% 

Epoch: 04 | Batch: 000 | CrossEntropy-loss: 0.46588 | Correct: 1048/1280 | Difference: 1.0542956277107869
Epoch: 04 | Batch: 100 | CrossEntropy-loss: 0.46817 | Correct: 1049/1280 | Difference: 1.0558554990964053
Epoch: 04 | Batch: 200 | CrossEntropy-loss: 0.48574 | Correct: 1040/1280 | Difference: 1.0564319917870078
Epoch: 04 | Batch: 300 | CrossEntropy-loss: 0.42596 | Correct: 1057/1280 | Difference: 1.0568137986568018
Epoch: 04 | Batch: 400 | CrossEntropy-loss: 0.48622 | Correct: 1026/1280 | Difference: 1.0576295848150368
Epoch: 04 | Batch: 500 | CrossEntropy-loss: 0.45689 | Correct: 1055/1280 | Difference: 1.0584633293950672
Epoch: 04 | Batch: 600 | CrossEntropy-loss: 0.46124 | Correct: 1050/1280 | Difference: 1.0597709580613601
Epoch: 04 | Batch: 700 | CrossEntropy-loss: 0.46170 | Correct: 1048/1280 | Difference: 1.060037812466893
Epoch: 04 | Batch: 800 | CrossEntropy-loss: 0.47174 | Correct: 1048/1280 | Difference: 1.0602778155073438
Epoch: 04 | Batch: 900 | CrossEntropy-loss: 0.46861 | Correct: 1045/1280 | Difference: 1.0606039776372052
Epoch: 04 | Batch: 1000 | CrossEntropy-loss: 0.45146 | Correct: 1042/1280 | Difference: 1.0615395566072419
Epoch: 04 | Batch: 1100 | CrossEntropy-loss: 0.46840 | Correct: 1045/1280 | Difference: 1.062278515441965
Epoch: 04 | Batch: 1200 | CrossEntropy-loss: 0.50824 | Correct: 1019/1280 | Difference: 1.0623969010189458
Epoch: 04 | Batch: 1300 | CrossEntropy-loss: 0.45709 | Correct: 1039/1280 | Difference: 1.0631172187347848
Epoch: 04 | Batch: 1400 | CrossEntropy-loss: 0.45208 | Correct: 1039/1280 | Difference: 1.0640713328776654
Epoch: 04 | Batch: 1500 | CrossEntropy-loss: 0.46934 | Correct: 1033/1280 | Difference: 1.0640109265368987

Epoch: 04 | Testing Accuracy: 412511/504607 (81.749%) | Historical Best: 81.749% 

Epoch: 05 | Batch: 000 | CrossEntropy-loss: 0.46185 | Correct: 1040/1280 | Difference: 1.0639723946369808
Epoch: 05 | Batch: 100 | CrossEntropy-loss: 0.44496 | Correct: 1060/1280 | Difference: 1.0646735252995898
Epoch: 05 | Batch: 200 | CrossEntropy-loss: 0.45398 | Correct: 1056/1280 | Difference: 1.064521069501863
Epoch: 05 | Batch: 300 | CrossEntropy-loss: 0.43696 | Correct: 1056/1280 | Difference: 1.0646397119699058
Epoch: 05 | Batch: 400 | CrossEntropy-loss: 0.47239 | Correct: 1038/1280 | Difference: 1.0652126816938194
Epoch: 05 | Batch: 500 | CrossEntropy-loss: 0.45326 | Correct: 1039/1280 | Difference: 1.0651864265777387
Epoch: 05 | Batch: 600 | CrossEntropy-loss: 0.44406 | Correct: 1063/1280 | Difference: 1.0646474675033504
Epoch: 05 | Batch: 700 | CrossEntropy-loss: 0.48469 | Correct: 1019/1280 | Difference: 1.0646340066407547
Epoch: 05 | Batch: 800 | CrossEntropy-loss: 0.45356 | Correct: 1061/1280 | Difference: 1.0661611176519536
Epoch: 05 | Batch: 900 | CrossEntropy-loss: 0.45431 | Correct: 1045/1280 | Difference: 1.0672189487220618
Epoch: 05 | Batch: 1000 | CrossEntropy-loss: 0.46432 | Correct: 1050/1280 | Difference: 1.067680999928931
Epoch: 05 | Batch: 1100 | CrossEntropy-loss: 0.50327 | Correct: 1026/1280 | Difference: 1.0684001250477522
Epoch: 05 | Batch: 1200 | CrossEntropy-loss: 0.43995 | Correct: 1051/1280 | Difference: 1.0685286919172867
Epoch: 05 | Batch: 1300 | CrossEntropy-loss: 0.40742 | Correct: 1071/1280 | Difference: 1.0686035335617439
Epoch: 05 | Batch: 1400 | CrossEntropy-loss: 0.44661 | Correct: 1057/1280 | Difference: 1.0687200263067367
Epoch: 05 | Batch: 1500 | CrossEntropy-loss: 0.44240 | Correct: 1061/1280 | Difference: 1.0691014858582246

Epoch: 05 | Testing Accuracy: 413185/504607 (81.883%) | Historical Best: 81.883% 

Epoch: 06 | Batch: 000 | CrossEntropy-loss: 0.47415 | Correct: 1034/1280 | Difference: 1.0694714827384058
Epoch: 06 | Batch: 100 | CrossEntropy-loss: 0.44878 | Correct: 1049/1280 | Difference: 1.0695265799937175
Epoch: 06 | Batch: 200 | CrossEntropy-loss: 0.43779 | Correct: 1057/1280 | Difference: 1.070120237922026
Epoch: 06 | Batch: 300 | CrossEntropy-loss: 0.44738 | Correct: 1058/1280 | Difference: 1.0701866850688637
Epoch: 06 | Batch: 400 | CrossEntropy-loss: 0.44940 | Correct: 1051/1280 | Difference: 1.0705586393187128
Epoch: 06 | Batch: 500 | CrossEntropy-loss: 0.45595 | Correct: 1043/1280 | Difference: 1.0719770076331507
Epoch: 06 | Batch: 600 | CrossEntropy-loss: 0.48413 | Correct: 1043/1280 | Difference: 1.0728037268520307
Epoch: 06 | Batch: 700 | CrossEntropy-loss: 0.45189 | Correct: 1065/1280 | Difference: 1.0732254918158224
Epoch: 06 | Batch: 800 | CrossEntropy-loss: 0.43891 | Correct: 1041/1280 | Difference: 1.0733883500205772
Epoch: 06 | Batch: 900 | CrossEntropy-loss: 0.43717 | Correct: 1055/1280 | Difference: 1.074021129559097
Epoch: 06 | Batch: 1000 | CrossEntropy-loss: 0.47961 | Correct: 1051/1280 | Difference: 1.0740295397181463
Epoch: 06 | Batch: 1100 | CrossEntropy-loss: 0.47220 | Correct: 1050/1280 | Difference: 1.0748041570236262
Epoch: 06 | Batch: 1200 | CrossEntropy-loss: 0.44282 | Correct: 1060/1280 | Difference: 1.0753533618590245
Epoch: 06 | Batch: 1300 | CrossEntropy-loss: 0.46549 | Correct: 1035/1280 | Difference: 1.0754554521455868
Epoch: 06 | Batch: 1400 | CrossEntropy-loss: 0.47133 | Correct: 1044/1280 | Difference: 1.0762033104149789
Epoch: 06 | Batch: 1500 | CrossEntropy-loss: 0.43471 | Correct: 1048/1280 | Difference: 1.0770877322227168

Epoch: 06 | Testing Accuracy: 414517/504607 (82.147%) | Historical Best: 82.147% 

Epoch: 07 | Batch: 000 | CrossEntropy-loss: 0.43518 | Correct: 1060/1280 | Difference: 1.0779316063902777
Epoch: 07 | Batch: 100 | CrossEntropy-loss: 0.48076 | Correct: 1049/1280 | Difference: 1.0786532302964977
Epoch: 07 | Batch: 200 | CrossEntropy-loss: 0.47269 | Correct: 1041/1280 | Difference: 1.0795673500414422
Epoch: 07 | Batch: 300 | CrossEntropy-loss: 0.43916 | Correct: 1059/1280 | Difference: 1.0797693926636422
Epoch: 07 | Batch: 400 | CrossEntropy-loss: 0.41464 | Correct: 1059/1280 | Difference: 1.0806908580762375
Epoch: 07 | Batch: 500 | CrossEntropy-loss: 0.46504 | Correct: 1058/1280 | Difference: 1.0809572328053456
Epoch: 07 | Batch: 600 | CrossEntropy-loss: 0.45442 | Correct: 1049/1280 | Difference: 1.0814293857968589
Epoch: 07 | Batch: 700 | CrossEntropy-loss: 0.44778 | Correct: 1055/1280 | Difference: 1.0820797795147774
Epoch: 07 | Batch: 800 | CrossEntropy-loss: 0.45508 | Correct: 1035/1280 | Difference: 1.0822693049209728
Epoch: 07 | Batch: 900 | CrossEntropy-loss: 0.42224 | Correct: 1082/1280 | Difference: 1.0829876069515174
Epoch: 07 | Batch: 1000 | CrossEntropy-loss: 0.44494 | Correct: 1056/1280 | Difference: 1.0841875890082646
Epoch: 07 | Batch: 1100 | CrossEntropy-loss: 0.45218 | Correct: 1064/1280 | Difference: 1.0844580155175856
Epoch: 07 | Batch: 1200 | CrossEntropy-loss: 0.44157 | Correct: 1055/1280 | Difference: 1.0850634859175474
Epoch: 07 | Batch: 1300 | CrossEntropy-loss: 0.47698 | Correct: 1025/1280 | Difference: 1.08643153935433
Epoch: 07 | Batch: 1400 | CrossEntropy-loss: 0.44573 | Correct: 1052/1280 | Difference: 1.0871567706955922
Epoch: 07 | Batch: 1500 | CrossEntropy-loss: 0.44334 | Correct: 1060/1280 | Difference: 1.0876005759506933

Epoch: 07 | Testing Accuracy: 416192/504607 (82.478%) | Historical Best: 82.478% 

Epoch: 08 | Batch: 000 | CrossEntropy-loss: 0.43600 | Correct: 1056/1280 | Difference: 1.0889120075428826
Epoch: 08 | Batch: 100 | CrossEntropy-loss: 0.47632 | Correct: 1041/1280 | Difference: 1.0895630968435226
Epoch: 08 | Batch: 200 | CrossEntropy-loss: 0.46352 | Correct: 1055/1280 | Difference: 1.089966157739597
Epoch: 08 | Batch: 300 | CrossEntropy-loss: 0.42826 | Correct: 1069/1280 | Difference: 1.090778481004483
Epoch: 08 | Batch: 400 | CrossEntropy-loss: 0.46601 | Correct: 1054/1280 | Difference: 1.0907678712436013
Epoch: 08 | Batch: 500 | CrossEntropy-loss: 0.43779 | Correct: 1064/1280 | Difference: 1.0922307094283048
Epoch: 08 | Batch: 600 | CrossEntropy-loss: 0.44616 | Correct: 1049/1280 | Difference: 1.0928816714515703
Epoch: 08 | Batch: 700 | CrossEntropy-loss: 0.43723 | Correct: 1056/1280 | Difference: 1.0933477115591494
Epoch: 08 | Batch: 800 | CrossEntropy-loss: 0.44217 | Correct: 1051/1280 | Difference: 1.0945911017016565
Epoch: 08 | Batch: 900 | CrossEntropy-loss: 0.40288 | Correct: 1078/1280 | Difference: 1.095071233579406
Epoch: 08 | Batch: 1000 | CrossEntropy-loss: 0.42022 | Correct: 1052/1280 | Difference: 1.0959145382073716
Epoch: 08 | Batch: 1100 | CrossEntropy-loss: 0.44705 | Correct: 1052/1280 | Difference: 1.0967599135045045
Epoch: 08 | Batch: 1200 | CrossEntropy-loss: 0.44629 | Correct: 1056/1280 | Difference: 1.097100464471561
Epoch: 08 | Batch: 1300 | CrossEntropy-loss: 0.47419 | Correct: 1039/1280 | Difference: 1.097513936376581
Epoch: 08 | Batch: 1400 | CrossEntropy-loss: 0.45103 | Correct: 1063/1280 | Difference: 1.0979784263843895
Epoch: 08 | Batch: 1500 | CrossEntropy-loss: 0.43156 | Correct: 1064/1280 | Difference: 1.0982451667843507

Epoch: 08 | Testing Accuracy: 417022/504607 (82.643%) | Historical Best: 82.643% 

Epoch: 09 | Batch: 000 | CrossEntropy-loss: 0.42390 | Correct: 1062/1280 | Difference: 1.098364987820927
Epoch: 09 | Batch: 100 | CrossEntropy-loss: 0.45978 | Correct: 1054/1280 | Difference: 1.0986562895991854
Epoch: 09 | Batch: 200 | CrossEntropy-loss: 0.45743 | Correct: 1052/1280 | Difference: 1.0992700911079734
Epoch: 09 | Batch: 300 | CrossEntropy-loss: 0.42357 | Correct: 1084/1280 | Difference: 1.0992436372319767
Epoch: 09 | Batch: 400 | CrossEntropy-loss: 0.44142 | Correct: 1065/1280 | Difference: 1.099385676533136
Epoch: 09 | Batch: 500 | CrossEntropy-loss: 0.45005 | Correct: 1037/1280 | Difference: 1.099918507698009
Epoch: 09 | Batch: 600 | CrossEntropy-loss: 0.41195 | Correct: 1083/1280 | Difference: 1.0998857545244034
Epoch: 09 | Batch: 700 | CrossEntropy-loss: 0.42214 | Correct: 1064/1280 | Difference: 1.0994611090914395
Epoch: 09 | Batch: 800 | CrossEntropy-loss: 0.45403 | Correct: 1061/1280 | Difference: 1.0992309082605174
Epoch: 09 | Batch: 900 | CrossEntropy-loss: 0.42962 | Correct: 1078/1280 | Difference: 1.0996517176947935
Epoch: 09 | Batch: 1000 | CrossEntropy-loss: 0.43106 | Correct: 1060/1280 | Difference: 1.099573137461465
Epoch: 09 | Batch: 1100 | CrossEntropy-loss: 0.43081 | Correct: 1063/1280 | Difference: 1.0997201263701328
Epoch: 09 | Batch: 1200 | CrossEntropy-loss: 0.43679 | Correct: 1060/1280 | Difference: 1.0994295280464779
Epoch: 09 | Batch: 1300 | CrossEntropy-loss: 0.46407 | Correct: 1036/1280 | Difference: 1.0993761394048527
Epoch: 09 | Batch: 1400 | CrossEntropy-loss: 0.45478 | Correct: 1050/1280 | Difference: 1.09919896405749
Epoch: 09 | Batch: 1500 | CrossEntropy-loss: 0.40535 | Correct: 1087/1280 | Difference: 1.099258944857069

Epoch: 09 | Testing Accuracy: 418219/504607 (82.880%) | Historical Best: 82.880% 

Epoch: 10 | Batch: 000 | CrossEntropy-loss: 0.45498 | Correct: 1049/1280 | Difference: 1.0988080802613476
Epoch: 10 | Batch: 100 | CrossEntropy-loss: 0.43550 | Correct: 1070/1280 | Difference: 1.098536958770241
Traceback (most recent call last):
  File "sdt_train.py", line 151, in <module>
    train_tree(tree)
  File "sdt_train.py", line 73, in train_tree
    prediction, output, penalty, weights = tree.forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 62, in forward
    _mu, _penalty, _alpha = self._forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 114, in _forward
    return mu, _penalty, torch.mean(torch.stack(half_alpha_list)).detach().cpu().numpy()   # mu contains the path probability for each leaf       
RuntimeError: expected a non-empty list of Tensors
