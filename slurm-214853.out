compute003
6
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
{'input_dim': 8, 'output_dim': 4, 'depth': 5, 'lamda': -0.01, 'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 1280, 'epochs': 40, 'cuda': True, 'log_interval': 100, 'exp_scheduler_gamma': 1.0, 'beta': True, 'greatest_path_probability': True, 'model_path': './model/trees/sdt_-0.01_id1'}
Epoch: 01 | Batch: 000 | CrossEntropy-loss: 1.38822 | Correct: 218/1280 | Difference: 0.9762496659224683
Epoch: 01 | Batch: 100 | CrossEntropy-loss: 1.27662 | Correct: 781/1280 | Difference: 0.9605353843975027
Epoch: 01 | Batch: 200 | CrossEntropy-loss: 1.14930 | Correct: 725/1280 | Difference: 0.9718449820416402
Epoch: 01 | Batch: 300 | CrossEntropy-loss: 1.04646 | Correct: 845/1280 | Difference: 0.9601472322001112
Epoch: 01 | Batch: 400 | CrossEntropy-loss: 0.93308 | Correct: 899/1280 | Difference: 0.9400669852815404
Epoch: 01 | Batch: 500 | CrossEntropy-loss: 0.87141 | Correct: 916/1280 | Difference: 0.9269494000642243
Epoch: 01 | Batch: 600 | CrossEntropy-loss: 0.81677 | Correct: 938/1280 | Difference: 0.912015575325994
Epoch: 01 | Batch: 700 | CrossEntropy-loss: 0.75866 | Correct: 955/1280 | Difference: 0.8976658701425502
Epoch: 01 | Batch: 800 | CrossEntropy-loss: 0.72089 | Correct: 942/1280 | Difference: 0.8856307200779113
Epoch: 01 | Batch: 900 | CrossEntropy-loss: 0.71067 | Correct: 953/1280 | Difference: 0.8742591812349197
Epoch: 01 | Batch: 1000 | CrossEntropy-loss: 0.67262 | Correct: 974/1280 | Difference: 0.864404071896773
Epoch: 01 | Batch: 1100 | CrossEntropy-loss: 0.65674 | Correct: 964/1280 | Difference: 0.8591687098560044
Epoch: 01 | Batch: 1200 | CrossEntropy-loss: 0.66576 | Correct: 948/1280 | Difference: 0.858344467695074
Epoch: 01 | Batch: 1300 | CrossEntropy-loss: 0.64349 | Correct: 966/1280 | Difference: 0.8605990629960064
Epoch: 01 | Batch: 1400 | CrossEntropy-loss: 0.61289 | Correct: 967/1280 | Difference: 0.8636021986098698
Epoch: 01 | Batch: 1500 | CrossEntropy-loss: 0.60672 | Correct: 974/1280 | Difference: 0.8672101045573063

Epoch: 01 | Testing Accuracy: 380256/504607 (75.357%) | Historical Best: 75.357% 

Epoch: 02 | Batch: 000 | CrossEntropy-loss: 0.57180 | Correct: 1004/1280 | Difference: 0.8706614277015196
Epoch: 02 | Batch: 100 | CrossEntropy-loss: 0.55594 | Correct: 1010/1280 | Difference: 0.8764391632319011
Epoch: 02 | Batch: 200 | CrossEntropy-loss: 0.60860 | Correct: 998/1280 | Difference: 0.884978537586807
Epoch: 02 | Batch: 300 | CrossEntropy-loss: 0.56274 | Correct: 1037/1280 | Difference: 0.8929684828166087
Epoch: 02 | Batch: 400 | CrossEntropy-loss: 0.54488 | Correct: 1055/1280 | Difference: 0.901760923518831
Epoch: 02 | Batch: 500 | CrossEntropy-loss: 0.57146 | Correct: 1009/1280 | Difference: 0.9104630653603385
Epoch: 02 | Batch: 600 | CrossEntropy-loss: 0.51974 | Correct: 1044/1280 | Difference: 0.92174510915207
Epoch: 02 | Batch: 700 | CrossEntropy-loss: 0.52782 | Correct: 1038/1280 | Difference: 0.9339493826078326
Epoch: 02 | Batch: 800 | CrossEntropy-loss: 0.48698 | Correct: 1056/1280 | Difference: 0.9459720448197703
Epoch: 02 | Batch: 900 | CrossEntropy-loss: 0.51300 | Correct: 1038/1280 | Difference: 0.9583940768161857
Epoch: 02 | Batch: 1000 | CrossEntropy-loss: 0.52878 | Correct: 1026/1280 | Difference: 0.97052707417768
Epoch: 02 | Batch: 1100 | CrossEntropy-loss: 0.55894 | Correct: 1026/1280 | Difference: 0.9832725118850024
Epoch: 02 | Batch: 1200 | CrossEntropy-loss: 0.49292 | Correct: 1046/1280 | Difference: 0.9980798349223289
Epoch: 02 | Batch: 1300 | CrossEntropy-loss: 0.53254 | Correct: 1016/1280 | Difference: 1.008077989418098
Epoch: 02 | Batch: 1400 | CrossEntropy-loss: 0.50008 | Correct: 1041/1280 | Difference: 1.011281634217245
Epoch: 02 | Batch: 1500 | CrossEntropy-loss: 0.47749 | Correct: 1048/1280 | Difference: 1.011945740761789

Epoch: 02 | Testing Accuracy: 407971/504607 (80.849%) | Historical Best: 80.849% 

Epoch: 03 | Batch: 000 | CrossEntropy-loss: 0.48234 | Correct: 1047/1280 | Difference: 1.012214549344935
Epoch: 03 | Batch: 100 | CrossEntropy-loss: 0.47987 | Correct: 1048/1280 | Difference: 1.0158481909250041
Epoch: 03 | Batch: 200 | CrossEntropy-loss: 0.49985 | Correct: 1032/1280 | Difference: 1.0264269905767758
Epoch: 03 | Batch: 300 | CrossEntropy-loss: 0.47426 | Correct: 1047/1280 | Difference: 1.0350782978158548
Epoch: 03 | Batch: 400 | CrossEntropy-loss: 0.47948 | Correct: 1051/1280 | Difference: 1.0418849054691037
Epoch: 03 | Batch: 500 | CrossEntropy-loss: 0.48864 | Correct: 1019/1280 | Difference: 1.0459781924165052
Epoch: 03 | Batch: 600 | CrossEntropy-loss: 0.47244 | Correct: 1052/1280 | Difference: 1.0495221730475761
Epoch: 03 | Batch: 700 | CrossEntropy-loss: 0.50119 | Correct: 1026/1280 | Difference: 1.051072788676692
Epoch: 03 | Batch: 800 | CrossEntropy-loss: 0.46876 | Correct: 1063/1280 | Difference: 1.0503320946098773
Epoch: 03 | Batch: 900 | CrossEntropy-loss: 0.46641 | Correct: 1036/1280 | Difference: 1.0499591107280641
Epoch: 03 | Batch: 1000 | CrossEntropy-loss: 0.49622 | Correct: 1036/1280 | Difference: 1.050910129359729
Epoch: 03 | Batch: 1100 | CrossEntropy-loss: 0.46583 | Correct: 1048/1280 | Difference: 1.0544427408347383
Epoch: 03 | Batch: 1200 | CrossEntropy-loss: 0.45552 | Correct: 1061/1280 | Difference: 1.055467588537545
Epoch: 03 | Batch: 1300 | CrossEntropy-loss: 0.44685 | Correct: 1044/1280 | Difference: 1.053305314943996
Epoch: 03 | Batch: 1400 | CrossEntropy-loss: 0.46716 | Correct: 1051/1280 | Difference: 1.0528347335931252
Epoch: 03 | Batch: 1500 | CrossEntropy-loss: 0.47945 | Correct: 1048/1280 | Difference: 1.0536651885421293

Epoch: 03 | Testing Accuracy: 411199/504607 (81.489%) | Historical Best: 81.489% 

Epoch: 04 | Batch: 000 | CrossEntropy-loss: 0.46588 | Correct: 1048/1280 | Difference: 1.0542956277107869
Epoch: 04 | Batch: 100 | CrossEntropy-loss: 0.46817 | Correct: 1049/1280 | Difference: 1.0558554990964053
Epoch: 04 | Batch: 200 | CrossEntropy-loss: 0.48574 | Correct: 1040/1280 | Difference: 1.0564319917870078
Epoch: 04 | Batch: 300 | CrossEntropy-loss: 0.42596 | Correct: 1057/1280 | Difference: 1.0568137986568018
Epoch: 04 | Batch: 400 | CrossEntropy-loss: 0.48622 | Correct: 1026/1280 | Difference: 1.0576295848150368
Epoch: 04 | Batch: 500 | CrossEntropy-loss: 0.45689 | Correct: 1055/1280 | Difference: 1.0584633293950672
Epoch: 04 | Batch: 600 | CrossEntropy-loss: 0.46124 | Correct: 1050/1280 | Difference: 1.0597709580613601
Epoch: 04 | Batch: 700 | CrossEntropy-loss: 0.46170 | Correct: 1048/1280 | Difference: 1.060037812466893
Epoch: 04 | Batch: 800 | CrossEntropy-loss: 0.47174 | Correct: 1048/1280 | Difference: 1.0602778155073438
Epoch: 04 | Batch: 900 | CrossEntropy-loss: 0.46861 | Correct: 1045/1280 | Difference: 1.0606039776372052
Epoch: 04 | Batch: 1000 | CrossEntropy-loss: 0.45146 | Correct: 1042/1280 | Difference: 1.0615395566072419
Epoch: 04 | Batch: 1100 | CrossEntropy-loss: 0.46840 | Correct: 1045/1280 | Difference: 1.062278515441965
Epoch: 04 | Batch: 1200 | CrossEntropy-loss: 0.50824 | Correct: 1019/1280 | Difference: 1.0623969010189458
Epoch: 04 | Batch: 1300 | CrossEntropy-loss: 0.45709 | Correct: 1039/1280 | Difference: 1.0631172187347848
Epoch: 04 | Batch: 1400 | CrossEntropy-loss: 0.45208 | Correct: 1039/1280 | Difference: 1.0640713328776654
Epoch: 04 | Batch: 1500 | CrossEntropy-loss: 0.46934 | Correct: 1033/1280 | Difference: 1.0640109265368987

Epoch: 04 | Testing Accuracy: 412511/504607 (81.749%) | Historical Best: 81.749% 

Epoch: 05 | Batch: 000 | CrossEntropy-loss: 0.46185 | Correct: 1040/1280 | Difference: 1.0639723946369808
Epoch: 05 | Batch: 100 | CrossEntropy-loss: 0.44496 | Correct: 1060/1280 | Difference: 1.0646735252995898
Epoch: 05 | Batch: 200 | CrossEntropy-loss: 0.45398 | Correct: 1056/1280 | Difference: 1.064521069501863
Epoch: 05 | Batch: 300 | CrossEntropy-loss: 0.43696 | Correct: 1056/1280 | Difference: 1.0646397119699058
Epoch: 05 | Batch: 400 | CrossEntropy-loss: 0.47239 | Correct: 1038/1280 | Difference: 1.0652126816938194
Epoch: 05 | Batch: 500 | CrossEntropy-loss: 0.45326 | Correct: 1039/1280 | Difference: 1.0651864265777387
Epoch: 05 | Batch: 600 | CrossEntropy-loss: 0.44406 | Correct: 1063/1280 | Difference: 1.0646474675033504
Epoch: 05 | Batch: 700 | CrossEntropy-loss: 0.48469 | Correct: 1019/1280 | Difference: 1.0646340066407547
Epoch: 05 | Batch: 800 | CrossEntropy-loss: 0.45356 | Correct: 1061/1280 | Difference: 1.0661611176519536
Epoch: 05 | Batch: 900 | CrossEntropy-loss: 0.45431 | Correct: 1045/1280 | Difference: 1.0672189487220618
Epoch: 05 | Batch: 1000 | CrossEntropy-loss: 0.46432 | Correct: 1050/1280 | Difference: 1.067680999928931
Epoch: 05 | Batch: 1100 | CrossEntropy-loss: 0.50327 | Correct: 1026/1280 | Difference: 1.0684001250477522
Epoch: 05 | Batch: 1200 | CrossEntropy-loss: 0.43995 | Correct: 1051/1280 | Difference: 1.0685286919172867
Epoch: 05 | Batch: 1300 | CrossEntropy-loss: 0.40742 | Correct: 1071/1280 | Difference: 1.0686035335617439
Epoch: 05 | Batch: 1400 | CrossEntropy-loss: 0.44661 | Correct: 1057/1280 | Difference: 1.0687200263067367
Epoch: 05 | Batch: 1500 | CrossEntropy-loss: 0.44240 | Correct: 1061/1280 | Difference: 1.0691014858582246

Epoch: 05 | Testing Accuracy: 413185/504607 (81.883%) | Historical Best: 81.883% 

Epoch: 06 | Batch: 000 | CrossEntropy-loss: 0.47415 | Correct: 1034/1280 | Difference: 1.0694714827384058
Epoch: 06 | Batch: 100 | CrossEntropy-loss: 0.44878 | Correct: 1049/1280 | Difference: 1.0695265799937175
Epoch: 06 | Batch: 200 | CrossEntropy-loss: 0.43779 | Correct: 1057/1280 | Difference: 1.070120237922026
Epoch: 06 | Batch: 300 | CrossEntropy-loss: 0.44738 | Correct: 1058/1280 | Difference: 1.0701866850688637
Epoch: 06 | Batch: 400 | CrossEntropy-loss: 0.44940 | Correct: 1051/1280 | Difference: 1.0705586393187128
Epoch: 06 | Batch: 500 | CrossEntropy-loss: 0.45595 | Correct: 1043/1280 | Difference: 1.0719770076331507
Epoch: 06 | Batch: 600 | CrossEntropy-loss: 0.48413 | Correct: 1043/1280 | Difference: 1.0728037268520307
Epoch: 06 | Batch: 700 | CrossEntropy-loss: 0.45189 | Correct: 1065/1280 | Difference: 1.0732254918158224
Epoch: 06 | Batch: 800 | CrossEntropy-loss: 0.43891 | Correct: 1041/1280 | Difference: 1.0733883500205772
Epoch: 06 | Batch: 900 | CrossEntropy-loss: 0.43717 | Correct: 1055/1280 | Difference: 1.074021129559097
Epoch: 06 | Batch: 1000 | CrossEntropy-loss: 0.47961 | Correct: 1051/1280 | Difference: 1.0740295397181463
Epoch: 06 | Batch: 1100 | CrossEntropy-loss: 0.47220 | Correct: 1050/1280 | Difference: 1.0748041570236262
Epoch: 06 | Batch: 1200 | CrossEntropy-loss: 0.44282 | Correct: 1060/1280 | Difference: 1.0753533618590245
Epoch: 06 | Batch: 1300 | CrossEntropy-loss: 0.46549 | Correct: 1035/1280 | Difference: 1.0754554521455868
Epoch: 06 | Batch: 1400 | CrossEntropy-loss: 0.47133 | Correct: 1044/1280 | Difference: 1.0762033104149789
Epoch: 06 | Batch: 1500 | CrossEntropy-loss: 0.43471 | Correct: 1048/1280 | Difference: 1.0770877322227168

Epoch: 06 | Testing Accuracy: 414517/504607 (82.147%) | Historical Best: 82.147% 

Epoch: 07 | Batch: 000 | CrossEntropy-loss: 0.43518 | Correct: 1060/1280 | Difference: 1.0779316063902777
Epoch: 07 | Batch: 100 | CrossEntropy-loss: 0.48076 | Correct: 1049/1280 | Difference: 1.0786532302964977
Epoch: 07 | Batch: 200 | CrossEntropy-loss: 0.47269 | Correct: 1041/1280 | Difference: 1.0795673500414422
Epoch: 07 | Batch: 300 | CrossEntropy-loss: 0.43916 | Correct: 1059/1280 | Difference: 1.0797693926636422
Epoch: 07 | Batch: 400 | CrossEntropy-loss: 0.41464 | Correct: 1059/1280 | Difference: 1.0806908580762375
Epoch: 07 | Batch: 500 | CrossEntropy-loss: 0.46504 | Correct: 1058/1280 | Difference: 1.0809572328053456
Epoch: 07 | Batch: 600 | CrossEntropy-loss: 0.45442 | Correct: 1049/1280 | Difference: 1.0814293857968589
Epoch: 07 | Batch: 700 | CrossEntropy-loss: 0.44778 | Correct: 1055/1280 | Difference: 1.0820797795147774
Epoch: 07 | Batch: 800 | CrossEntropy-loss: 0.45508 | Correct: 1035/1280 | Difference: 1.0822693049209728
Epoch: 07 | Batch: 900 | CrossEntropy-loss: 0.42224 | Correct: 1082/1280 | Difference: 1.0829876069515174
Epoch: 07 | Batch: 1000 | CrossEntropy-loss: 0.44494 | Correct: 1056/1280 | Difference: 1.0841875890082646
Epoch: 07 | Batch: 1100 | CrossEntropy-loss: 0.45218 | Correct: 1064/1280 | Difference: 1.0844580155175856
Epoch: 07 | Batch: 1200 | CrossEntropy-loss: 0.44157 | Correct: 1055/1280 | Difference: 1.0850634859175474
Epoch: 07 | Batch: 1300 | CrossEntropy-loss: 0.47698 | Correct: 1025/1280 | Difference: 1.08643153935433
Epoch: 07 | Batch: 1400 | CrossEntropy-loss: 0.44573 | Correct: 1052/1280 | Difference: 1.0871567706955922
Epoch: 07 | Batch: 1500 | CrossEntropy-loss: 0.44334 | Correct: 1060/1280 | Difference: 1.0876005759506933

Epoch: 07 | Testing Accuracy: 416192/504607 (82.478%) | Historical Best: 82.478% 

Epoch: 08 | Batch: 000 | CrossEntropy-loss: 0.43600 | Correct: 1056/1280 | Difference: 1.0889120075428826
Epoch: 08 | Batch: 100 | CrossEntropy-loss: 0.47632 | Correct: 1041/1280 | Difference: 1.0895630968435226
Epoch: 08 | Batch: 200 | CrossEntropy-loss: 0.46352 | Correct: 1055/1280 | Difference: 1.089966157739597
Epoch: 08 | Batch: 300 | CrossEntropy-loss: 0.42826 | Correct: 1069/1280 | Difference: 1.090778481004483
Epoch: 08 | Batch: 400 | CrossEntropy-loss: 0.46601 | Correct: 1054/1280 | Difference: 1.0907678712436013
Epoch: 08 | Batch: 500 | CrossEntropy-loss: 0.43779 | Correct: 1064/1280 | Difference: 1.0922307094283048
Epoch: 08 | Batch: 600 | CrossEntropy-loss: 0.44616 | Correct: 1049/1280 | Difference: 1.0928816714515703
Epoch: 08 | Batch: 700 | CrossEntropy-loss: 0.43723 | Correct: 1056/1280 | Difference: 1.0933477115591494
Epoch: 08 | Batch: 800 | CrossEntropy-loss: 0.44217 | Correct: 1051/1280 | Difference: 1.0945911017016565
Epoch: 08 | Batch: 900 | CrossEntropy-loss: 0.40288 | Correct: 1078/1280 | Difference: 1.095071233579406
Epoch: 08 | Batch: 1000 | CrossEntropy-loss: 0.42022 | Correct: 1052/1280 | Difference: 1.0959145382073716
Epoch: 08 | Batch: 1100 | CrossEntropy-loss: 0.44705 | Correct: 1052/1280 | Difference: 1.0967599135045045
Epoch: 08 | Batch: 1200 | CrossEntropy-loss: 0.44629 | Correct: 1056/1280 | Difference: 1.097100464471561
Epoch: 08 | Batch: 1300 | CrossEntropy-loss: 0.47419 | Correct: 1039/1280 | Difference: 1.097513936376581
Epoch: 08 | Batch: 1400 | CrossEntropy-loss: 0.45103 | Correct: 1063/1280 | Difference: 1.0979784263843895
Epoch: 08 | Batch: 1500 | CrossEntropy-loss: 0.43156 | Correct: 1064/1280 | Difference: 1.0982451667843507

Epoch: 08 | Testing Accuracy: 417022/504607 (82.643%) | Historical Best: 82.643% 

Epoch: 09 | Batch: 000 | CrossEntropy-loss: 0.42390 | Correct: 1062/1280 | Difference: 1.098364987820927
Epoch: 09 | Batch: 100 | CrossEntropy-loss: 0.45978 | Correct: 1054/1280 | Difference: 1.0986562895991854
Epoch: 09 | Batch: 200 | CrossEntropy-loss: 0.45743 | Correct: 1052/1280 | Difference: 1.0992700911079734
Epoch: 09 | Batch: 300 | CrossEntropy-loss: 0.42357 | Correct: 1084/1280 | Difference: 1.0992436372319767
Epoch: 09 | Batch: 400 | CrossEntropy-loss: 0.44142 | Correct: 1065/1280 | Difference: 1.099385676533136
Epoch: 09 | Batch: 500 | CrossEntropy-loss: 0.45005 | Correct: 1037/1280 | Difference: 1.099918507698009
Epoch: 09 | Batch: 600 | CrossEntropy-loss: 0.41195 | Correct: 1083/1280 | Difference: 1.0998857545244034
Epoch: 09 | Batch: 700 | CrossEntropy-loss: 0.42214 | Correct: 1064/1280 | Difference: 1.0994611090914395
Epoch: 09 | Batch: 800 | CrossEntropy-loss: 0.45403 | Correct: 1061/1280 | Difference: 1.0992309082605174
Epoch: 09 | Batch: 900 | CrossEntropy-loss: 0.42962 | Correct: 1078/1280 | Difference: 1.0996517176947935
Epoch: 09 | Batch: 1000 | CrossEntropy-loss: 0.43106 | Correct: 1060/1280 | Difference: 1.099573137461465
Epoch: 09 | Batch: 1100 | CrossEntropy-loss: 0.43081 | Correct: 1063/1280 | Difference: 1.0997201263701328
Epoch: 09 | Batch: 1200 | CrossEntropy-loss: 0.43679 | Correct: 1060/1280 | Difference: 1.0994295280464779
Epoch: 09 | Batch: 1300 | CrossEntropy-loss: 0.46407 | Correct: 1036/1280 | Difference: 1.0993761394048527
Epoch: 09 | Batch: 1400 | CrossEntropy-loss: 0.45478 | Correct: 1050/1280 | Difference: 1.09919896405749
Epoch: 09 | Batch: 1500 | CrossEntropy-loss: 0.40535 | Correct: 1087/1280 | Difference: 1.099258944857069

Epoch: 09 | Testing Accuracy: 418219/504607 (82.880%) | Historical Best: 82.880% 

Epoch: 10 | Batch: 000 | CrossEntropy-loss: 0.45498 | Correct: 1049/1280 | Difference: 1.0988080802613476
Epoch: 10 | Batch: 100 | CrossEntropy-loss: 0.43550 | Correct: 1070/1280 | Difference: 1.098536958770241
Traceback (most recent call last):
  File "sdt_train.py", line 151, in <module>
    train_tree(tree)
  File "sdt_train.py", line 73, in train_tree
    prediction, output, penalty, weights = tree.forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 62, in forward
    _mu, _penalty, _alpha = self._forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 114, in _forward
    return mu, _penalty, torch.mean(torch.stack(half_alpha_list)).detach().cpu().numpy()   # mu contains the path probability for each leaf       
RuntimeError: expected a non-empty list of Tensors
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
{'input_dim': 8, 'output_dim': 4, 'depth': 5, 'lamda': -0.01, 'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 1280, 'epochs': 40, 'cuda': True, 'log_interval': 100, 'exp_scheduler_gamma': 1.0, 'beta': True, 'greatest_path_probability': True, 'model_path': './model/trees/sdt_-0.01_id2'}
Epoch: 01 | Batch: 000 | CrossEntropy-loss: 1.41048 | Correct: 80/1280 | Difference: 0.9638989587758687
Epoch: 01 | Batch: 100 | CrossEntropy-loss: 1.28788 | Correct: 466/1280 | Difference: 0.9555316368899798
Epoch: 01 | Batch: 200 | CrossEntropy-loss: 1.17479 | Correct: 644/1280 | Difference: 0.9884389669021104
Epoch: 01 | Batch: 300 | CrossEntropy-loss: 1.05222 | Correct: 678/1280 | Difference: 0.9684830808100988
Epoch: 01 | Batch: 400 | CrossEntropy-loss: 0.95009 | Correct: 684/1280 | Difference: 0.9595222479239832
Epoch: 01 | Batch: 500 | CrossEntropy-loss: 0.87137 | Correct: 789/1280 | Difference: 0.9556818036710442
Epoch: 01 | Batch: 600 | CrossEntropy-loss: 0.81822 | Correct: 858/1280 | Difference: 0.9393366360290443
Epoch: 01 | Batch: 700 | CrossEntropy-loss: 0.75741 | Correct: 901/1280 | Difference: 0.9233532407191617
Epoch: 01 | Batch: 800 | CrossEntropy-loss: 0.72403 | Correct: 934/1280 | Difference: 0.9086531065976009
Epoch: 01 | Batch: 900 | CrossEntropy-loss: 0.69139 | Correct: 936/1280 | Difference: 0.8909895144977924
Epoch: 01 | Batch: 1000 | CrossEntropy-loss: 0.66617 | Correct: 968/1280 | Difference: 0.8764207463648698
Epoch: 01 | Batch: 1100 | CrossEntropy-loss: 0.67773 | Correct: 955/1280 | Difference: 0.8636250111817565
Epoch: 01 | Batch: 1200 | CrossEntropy-loss: 0.66244 | Correct: 971/1280 | Difference: 0.8530255508323561
Epoch: 01 | Batch: 1300 | CrossEntropy-loss: 0.60965 | Correct: 1003/1280 | Difference: 0.8473051367464436
Epoch: 01 | Batch: 1400 | CrossEntropy-loss: 0.59974 | Correct: 982/1280 | Difference: 0.8446626278352742
Epoch: 01 | Batch: 1500 | CrossEntropy-loss: 0.62292 | Correct: 995/1280 | Difference: 0.8443143992686717

Epoch: 01 | Testing Accuracy: 391480/504607 (77.581%) | Historical Best: 77.581% 

Epoch: 02 | Batch: 000 | CrossEntropy-loss: 0.56893 | Correct: 1007/1280 | Difference: 0.84732222562378
Epoch: 02 | Batch: 100 | CrossEntropy-loss: 0.58369 | Correct: 1018/1280 | Difference: 0.8540139586404806
Epoch: 02 | Batch: 200 | CrossEntropy-loss: 0.56205 | Correct: 1018/1280 | Difference: 0.8616237308996458
Epoch: 02 | Batch: 300 | CrossEntropy-loss: 0.54456 | Correct: 1034/1280 | Difference: 0.8713662843141293
Epoch: 02 | Batch: 400 | CrossEntropy-loss: 0.54415 | Correct: 1005/1280 | Difference: 0.8825247185949969
Epoch: 02 | Batch: 500 | CrossEntropy-loss: 0.55037 | Correct: 1009/1280 | Difference: 0.8947648831703625
Epoch: 02 | Batch: 600 | CrossEntropy-loss: 0.55800 | Correct: 1004/1280 | Difference: 0.9074995429419332
Epoch: 02 | Batch: 700 | CrossEntropy-loss: 0.50950 | Correct: 1023/1280 | Difference: 0.9198109096349283
Epoch: 02 | Batch: 800 | CrossEntropy-loss: 0.53146 | Correct: 1024/1280 | Difference: 0.9326867160347486
Epoch: 02 | Batch: 900 | CrossEntropy-loss: 0.49091 | Correct: 1027/1280 | Difference: 0.9454207340088979
Epoch: 02 | Batch: 1000 | CrossEntropy-loss: 0.54915 | Correct: 1000/1280 | Difference: 0.9591686829736348
Epoch: 02 | Batch: 1100 | CrossEntropy-loss: 0.53240 | Correct: 1030/1280 | Difference: 0.9702739421318493
Epoch: 02 | Batch: 1200 | CrossEntropy-loss: 0.51398 | Correct: 1033/1280 | Difference: 0.9794729657059121
Epoch: 02 | Batch: 1300 | CrossEntropy-loss: 0.49878 | Correct: 1051/1280 | Difference: 0.99063764201986
Epoch: 02 | Batch: 1400 | CrossEntropy-loss: 0.50780 | Correct: 1026/1280 | Difference: 1.0043632649196406
Epoch: 02 | Batch: 1500 | CrossEntropy-loss: 0.49884 | Correct: 1039/1280 | Difference: 1.0167043343769415

Epoch: 02 | Testing Accuracy: 408314/504607 (80.917%) | Historical Best: 80.917% 

Epoch: 03 | Batch: 000 | CrossEntropy-loss: 0.52291 | Correct: 1034/1280 | Difference: 1.0234711541466877
Epoch: 03 | Batch: 100 | CrossEntropy-loss: 0.51920 | Correct: 1017/1280 | Difference: 1.0327140121107106
Epoch: 03 | Batch: 200 | CrossEntropy-loss: 0.48017 | Correct: 1047/1280 | Difference: 1.0443698535310566
Epoch: 03 | Batch: 300 | CrossEntropy-loss: 0.48458 | Correct: 1053/1280 | Difference: 1.053103406081072
Epoch: 03 | Batch: 400 | CrossEntropy-loss: 0.49434 | Correct: 1055/1280 | Difference: 1.0573245377523737
Epoch: 03 | Batch: 500 | CrossEntropy-loss: 0.49339 | Correct: 1035/1280 | Difference: 1.0597915838207834
Epoch: 03 | Batch: 600 | CrossEntropy-loss: 0.46450 | Correct: 1046/1280 | Difference: 1.0620214656446239
Epoch: 03 | Batch: 700 | CrossEntropy-loss: 0.44599 | Correct: 1056/1280 | Difference: 1.0634360642730618
Epoch: 03 | Batch: 800 | CrossEntropy-loss: 0.47961 | Correct: 1040/1280 | Difference: 1.0641278686643674
Epoch: 03 | Batch: 900 | CrossEntropy-loss: 0.46220 | Correct: 1051/1280 | Difference: 1.0652726386976301
Epoch: 03 | Batch: 1000 | CrossEntropy-loss: 0.47494 | Correct: 1041/1280 | Difference: 1.0678143882254882
Epoch: 03 | Batch: 1100 | CrossEntropy-loss: 0.46566 | Correct: 1037/1280 | Difference: 1.0683693112911636
Epoch: 03 | Batch: 1200 | CrossEntropy-loss: 0.46923 | Correct: 1045/1280 | Difference: 1.0693155040018203
Epoch: 03 | Batch: 1300 | CrossEntropy-loss: 0.46396 | Correct: 1048/1280 | Difference: 1.0695668755910128
Epoch: 03 | Batch: 1400 | CrossEntropy-loss: 0.45994 | Correct: 1050/1280 | Difference: 1.072029142145239
Epoch: 03 | Batch: 1500 | CrossEntropy-loss: 0.48408 | Correct: 1032/1280 | Difference: 1.072835137513634

Epoch: 03 | Testing Accuracy: 411335/504607 (81.516%) | Historical Best: 81.516% 

Epoch: 04 | Batch: 000 | CrossEntropy-loss: 0.47633 | Correct: 1041/1280 | Difference: 1.073389323607533
Epoch: 04 | Batch: 100 | CrossEntropy-loss: 0.47341 | Correct: 1046/1280 | Difference: 1.073626539589257
Epoch: 04 | Batch: 200 | CrossEntropy-loss: 0.47990 | Correct: 1034/1280 | Difference: 1.073322473962914
Epoch: 04 | Batch: 300 | CrossEntropy-loss: 0.48103 | Correct: 1039/1280 | Difference: 1.0739839651906034
Epoch: 04 | Batch: 400 | CrossEntropy-loss: 0.47936 | Correct: 1044/1280 | Difference: 1.074501500934275
Epoch: 04 | Batch: 500 | CrossEntropy-loss: 0.48039 | Correct: 1048/1280 | Difference: 1.0741800341233865
Epoch: 04 | Batch: 600 | CrossEntropy-loss: 0.47957 | Correct: 1024/1280 | Difference: 1.0744628675518721
Epoch: 04 | Batch: 700 | CrossEntropy-loss: 0.47893 | Correct: 1027/1280 | Difference: 1.0749044296156276
Epoch: 04 | Batch: 800 | CrossEntropy-loss: 0.46356 | Correct: 1042/1280 | Difference: 1.0751538308595232
Epoch: 04 | Batch: 900 | CrossEntropy-loss: 0.47259 | Correct: 1051/1280 | Difference: 1.0758188120362806
Epoch: 04 | Batch: 1000 | CrossEntropy-loss: 0.48085 | Correct: 1047/1280 | Difference: 1.0760618686992076
Epoch: 04 | Batch: 1100 | CrossEntropy-loss: 0.46388 | Correct: 1044/1280 | Difference: 1.076965425298748
Epoch: 04 | Batch: 1200 | CrossEntropy-loss: 0.45974 | Correct: 1056/1280 | Difference: 1.0775531842536077
Epoch: 04 | Batch: 1300 | CrossEntropy-loss: 0.48999 | Correct: 1025/1280 | Difference: 1.077936285812991
Epoch: 04 | Batch: 1400 | CrossEntropy-loss: 0.48410 | Correct: 1020/1280 | Difference: 1.0787105795237673
Epoch: 04 | Batch: 1500 | CrossEntropy-loss: 0.45964 | Correct: 1043/1280 | Difference: 1.0792877848154387

Epoch: 04 | Testing Accuracy: 412441/504607 (81.735%) | Historical Best: 81.735% 

Epoch: 05 | Batch: 000 | CrossEntropy-loss: 0.44106 | Correct: 1058/1280 | Difference: 1.0796450983925003
Epoch: 05 | Batch: 100 | CrossEntropy-loss: 0.48350 | Correct: 1044/1280 | Difference: 1.0804368970320999
Epoch: 05 | Batch: 200 | CrossEntropy-loss: 0.45207 | Correct: 1053/1280 | Difference: 1.080706446633373
Epoch: 05 | Batch: 300 | CrossEntropy-loss: 0.43416 | Correct: 1066/1280 | Difference: 1.0822048512607185
Epoch: 05 | Batch: 400 | CrossEntropy-loss: 0.42707 | Correct: 1056/1280 | Difference: 1.0828196345630785
Epoch: 05 | Batch: 500 | CrossEntropy-loss: 0.46949 | Correct: 1052/1280 | Difference: 1.0832184392921442
Epoch: 05 | Batch: 600 | CrossEntropy-loss: 0.44064 | Correct: 1054/1280 | Difference: 1.0837134250394755
Epoch: 05 | Batch: 700 | CrossEntropy-loss: 0.46221 | Correct: 1044/1280 | Difference: 1.084247033517902
Epoch: 05 | Batch: 800 | CrossEntropy-loss: 0.48989 | Correct: 1024/1280 | Difference: 1.0843306263499528
Epoch: 05 | Batch: 900 | CrossEntropy-loss: 0.44588 | Correct: 1060/1280 | Difference: 1.084592057473785
Epoch: 05 | Batch: 1000 | CrossEntropy-loss: 0.46766 | Correct: 1040/1280 | Difference: 1.0847040935145476
Epoch: 05 | Batch: 1100 | CrossEntropy-loss: 0.47367 | Correct: 1049/1280 | Difference: 1.0861275760254248
Epoch: 05 | Batch: 1200 | CrossEntropy-loss: 0.48184 | Correct: 1022/1280 | Difference: 1.0865678532054563
Epoch: 05 | Batch: 1300 | CrossEntropy-loss: 0.48126 | Correct: 1029/1280 | Difference: 1.0865452885710234
Epoch: 05 | Batch: 1400 | CrossEntropy-loss: 0.46598 | Correct: 1051/1280 | Difference: 1.0870212557791197
Epoch: 05 | Batch: 1500 | CrossEntropy-loss: 0.44771 | Correct: 1051/1280 | Difference: 1.0871187692746436

Epoch: 05 | Testing Accuracy: 414460/504607 (82.135%) | Historical Best: 82.135% 

Epoch: 06 | Batch: 000 | CrossEntropy-loss: 0.47262 | Correct: 1051/1280 | Difference: 1.0875476279590643
Epoch: 06 | Batch: 100 | CrossEntropy-loss: 0.46107 | Correct: 1044/1280 | Difference: 1.088164672065671
Epoch: 06 | Batch: 200 | CrossEntropy-loss: 0.46403 | Correct: 1043/1280 | Difference: 1.0885543325977765
Epoch: 06 | Batch: 300 | CrossEntropy-loss: 0.46260 | Correct: 1046/1280 | Difference: 1.0889930555448482
Epoch: 06 | Batch: 400 | CrossEntropy-loss: 0.41518 | Correct: 1066/1280 | Difference: 1.0893824420953715
Epoch: 06 | Batch: 500 | CrossEntropy-loss: 0.45792 | Correct: 1054/1280 | Difference: 1.0899869935828432
Epoch: 06 | Batch: 600 | CrossEntropy-loss: 0.44765 | Correct: 1052/1280 | Difference: 1.0907765766839534
Epoch: 06 | Batch: 700 | CrossEntropy-loss: 0.42727 | Correct: 1060/1280 | Difference: 1.0908621366694586
Epoch: 06 | Batch: 800 | CrossEntropy-loss: 0.43654 | Correct: 1066/1280 | Difference: 1.0916730355055468
Epoch: 06 | Batch: 900 | CrossEntropy-loss: 0.41866 | Correct: 1070/1280 | Difference: 1.0926495690153153
Epoch: 06 | Batch: 1000 | CrossEntropy-loss: 0.47443 | Correct: 1019/1280 | Difference: 1.093128321578863
Epoch: 06 | Batch: 1100 | CrossEntropy-loss: 0.45213 | Correct: 1047/1280 | Difference: 1.093528170722847
Epoch: 06 | Batch: 1200 | CrossEntropy-loss: 0.44130 | Correct: 1060/1280 | Difference: 1.0946648736722182
Epoch: 06 | Batch: 1300 | CrossEntropy-loss: 0.42641 | Correct: 1066/1280 | Difference: 1.095171817762887
Epoch: 06 | Batch: 1400 | CrossEntropy-loss: 0.47396 | Correct: 1039/1280 | Difference: 1.0959959187279502
Epoch: 06 | Batch: 1500 | CrossEntropy-loss: 0.41822 | Correct: 1061/1280 | Difference: 1.0971032867040382

Epoch: 06 | Testing Accuracy: 414049/504607 (82.054%) | Historical Best: 82.135% 

Epoch: 07 | Batch: 000 | CrossEntropy-loss: 0.48009 | Correct: 1034/1280 | Difference: 1.0974448830149663
Epoch: 07 | Batch: 100 | CrossEntropy-loss: 0.40953 | Correct: 1066/1280 | Difference: 1.0975820963846776
Epoch: 07 | Batch: 200 | CrossEntropy-loss: 0.45895 | Correct: 1055/1280 | Difference: 1.0985866299302123
Epoch: 07 | Batch: 300 | CrossEntropy-loss: 0.46283 | Correct: 1032/1280 | Difference: 1.0995830364462347
Epoch: 07 | Batch: 400 | CrossEntropy-loss: 0.41282 | Correct: 1082/1280 | Difference: 1.0999780463074422
Epoch: 07 | Batch: 500 | CrossEntropy-loss: 0.42399 | Correct: 1063/1280 | Difference: 1.1006220034929575
Epoch: 07 | Batch: 600 | CrossEntropy-loss: 0.47066 | Correct: 1047/1280 | Difference: 1.101382927206477
Epoch: 07 | Batch: 700 | CrossEntropy-loss: 0.44203 | Correct: 1048/1280 | Difference: 1.1020056054165945
Epoch: 07 | Batch: 800 | CrossEntropy-loss: 0.43917 | Correct: 1062/1280 | Difference: 1.1026191575471092
Epoch: 07 | Batch: 900 | CrossEntropy-loss: 0.44017 | Correct: 1067/1280 | Difference: 1.1028340300589945
Epoch: 07 | Batch: 1000 | CrossEntropy-loss: 0.44408 | Correct: 1056/1280 | Difference: 1.1030816422151788
Epoch: 07 | Batch: 1100 | CrossEntropy-loss: 0.44136 | Correct: 1046/1280 | Difference: 1.1038567364532947
Epoch: 07 | Batch: 1200 | CrossEntropy-loss: 0.42969 | Correct: 1060/1280 | Difference: 1.1043660889328755
Epoch: 07 | Batch: 1300 | CrossEntropy-loss: 0.45156 | Correct: 1045/1280 | Difference: 1.104435674663077
Epoch: 07 | Batch: 1400 | CrossEntropy-loss: 0.45154 | Correct: 1052/1280 | Difference: 1.104977757880785
Epoch: 07 | Batch: 1500 | CrossEntropy-loss: 0.45990 | Correct: 1049/1280 | Difference: 1.1053977564879776

Epoch: 07 | Testing Accuracy: 414344/504607 (82.112%) | Historical Best: 82.135% 

Epoch: 08 | Batch: 000 | CrossEntropy-loss: 0.41342 | Correct: 1081/1280 | Difference: 1.1060639369615766
Epoch: 08 | Batch: 100 | CrossEntropy-loss: 0.44989 | Correct: 1043/1280 | Difference: 1.1065210181486274
Epoch: 08 | Batch: 200 | CrossEntropy-loss: 0.46736 | Correct: 1052/1280 | Difference: 1.1063618353633495
Epoch: 08 | Batch: 300 | CrossEntropy-loss: 0.50825 | Correct: 1003/1280 | Difference: 1.1068594381482393
Epoch: 08 | Batch: 400 | CrossEntropy-loss: 0.46484 | Correct: 1050/1280 | Difference: 1.1071381628427661
Epoch: 08 | Batch: 500 | CrossEntropy-loss: 0.45958 | Correct: 1050/1280 | Difference: 1.1071993114804841
Epoch: 08 | Batch: 600 | CrossEntropy-loss: 0.42280 | Correct: 1077/1280 | Difference: 1.1080149404759576
Epoch: 08 | Batch: 700 | CrossEntropy-loss: 0.45798 | Correct: 1035/1280 | Difference: 1.108376153191151
Epoch: 08 | Batch: 800 | CrossEntropy-loss: 0.46452 | Correct: 1049/1280 | Difference: 1.1098001053088764
Epoch: 08 | Batch: 900 | CrossEntropy-loss: 0.42315 | Correct: 1065/1280 | Difference: 1.1101326344265752
Traceback (most recent call last):
  File "sdt_train.py", line 151, in <module>
    train_tree(tree)
  File "sdt_train.py", line 73, in train_tree
    prediction, output, penalty, weights = tree.forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 62, in forward
    _mu, _penalty, _alpha = self._forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 114, in _forward
    return mu, _penalty, torch.mean(torch.stack(half_alpha_list)).detach().cpu().numpy()   # mu contains the path probability for each leaf       
RuntimeError: expected a non-empty list of Tensors
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
{'input_dim': 8, 'output_dim': 4, 'depth': 5, 'lamda': -0.01, 'lr': 0.001, 'weight_decay': 0.0, 'batch_size': 1280, 'epochs': 40, 'cuda': True, 'log_interval': 100, 'exp_scheduler_gamma': 1.0, 'beta': True, 'greatest_path_probability': True, 'model_path': './model/trees/sdt_-0.01_id3'}
Epoch: 01 | Batch: 000 | CrossEntropy-loss: 1.41814 | Correct: 268/1280 | Difference: 0.9849415629837825
Epoch: 01 | Batch: 100 | CrossEntropy-loss: 1.30782 | Correct: 768/1280 | Difference: 0.9781058621696447
Epoch: 01 | Batch: 200 | CrossEntropy-loss: 1.19955 | Correct: 841/1280 | Difference: 0.972916316874773
Epoch: 01 | Batch: 300 | CrossEntropy-loss: 1.10402 | Correct: 833/1280 | Difference: 0.9440155803480115
Epoch: 01 | Batch: 400 | CrossEntropy-loss: 1.01870 | Correct: 828/1280 | Difference: 0.9335263963384474
Epoch: 01 | Batch: 500 | CrossEntropy-loss: 0.91436 | Correct: 914/1280 | Difference: 0.9284646883999986
Epoch: 01 | Batch: 600 | CrossEntropy-loss: 0.83887 | Correct: 926/1280 | Difference: 0.9218697786448792
Epoch: 01 | Batch: 700 | CrossEntropy-loss: 0.81341 | Correct: 929/1280 | Difference: 0.9030217076446118
Epoch: 01 | Batch: 800 | CrossEntropy-loss: 0.74352 | Correct: 964/1280 | Difference: 0.8879351582724169
Epoch: 01 | Batch: 900 | CrossEntropy-loss: 0.73514 | Correct: 942/1280 | Difference: 0.8719003216056712
Epoch: 01 | Batch: 1000 | CrossEntropy-loss: 0.73059 | Correct: 956/1280 | Difference: 0.8582135662705627
Epoch: 01 | Batch: 1100 | CrossEntropy-loss: 0.67989 | Correct: 962/1280 | Difference: 0.8496063834358891
Epoch: 01 | Batch: 1200 | CrossEntropy-loss: 0.66999 | Correct: 950/1280 | Difference: 0.8407870892852363
Epoch: 01 | Batch: 1300 | CrossEntropy-loss: 0.67881 | Correct: 970/1280 | Difference: 0.83435455629252
Epoch: 01 | Batch: 1400 | CrossEntropy-loss: 0.64174 | Correct: 968/1280 | Difference: 0.8337917361873176
Epoch: 01 | Batch: 1500 | CrossEntropy-loss: 0.65953 | Correct: 932/1280 | Difference: 0.8351263290389611

Epoch: 01 | Testing Accuracy: 377950/504607 (74.900%) | Historical Best: 74.900% 

Epoch: 02 | Batch: 000 | CrossEntropy-loss: 0.66508 | Correct: 947/1280 | Difference: 0.837418955807171
Epoch: 02 | Batch: 100 | CrossEntropy-loss: 0.65733 | Correct: 960/1280 | Difference: 0.8413672634911812
Epoch: 02 | Batch: 200 | CrossEntropy-loss: 0.64146 | Correct: 946/1280 | Difference: 0.8459641262680069
Epoch: 02 | Batch: 300 | CrossEntropy-loss: 0.58785 | Correct: 961/1280 | Difference: 0.8553767339756643
Epoch: 02 | Batch: 400 | CrossEntropy-loss: 0.62199 | Correct: 947/1280 | Difference: 0.8641185000949666
Epoch: 02 | Batch: 500 | CrossEntropy-loss: 0.56587 | Correct: 1011/1280 | Difference: 0.8729687237799033
Epoch: 02 | Batch: 600 | CrossEntropy-loss: 0.58577 | Correct: 990/1280 | Difference: 0.8823737055589176
Epoch: 02 | Batch: 700 | CrossEntropy-loss: 0.57008 | Correct: 1012/1280 | Difference: 0.8960614034444825
Epoch: 02 | Batch: 800 | CrossEntropy-loss: 0.57335 | Correct: 1005/1280 | Difference: 0.9141305888382031
Epoch: 02 | Batch: 900 | CrossEntropy-loss: 0.56124 | Correct: 992/1280 | Difference: 0.9349250063117622
Epoch: 02 | Batch: 1000 | CrossEntropy-loss: 0.56306 | Correct: 986/1280 | Difference: 0.9558980138499604
Epoch: 02 | Batch: 1100 | CrossEntropy-loss: 0.56355 | Correct: 983/1280 | Difference: 0.9747232434533644
Epoch: 02 | Batch: 1200 | CrossEntropy-loss: 0.56067 | Correct: 982/1280 | Difference: 0.9941876776740407
Epoch: 02 | Batch: 1300 | CrossEntropy-loss: 0.52744 | Correct: 997/1280 | Difference: 1.0121713087213906
Epoch: 02 | Batch: 1400 | CrossEntropy-loss: 0.54250 | Correct: 1029/1280 | Difference: 1.0283105903307075
Epoch: 02 | Batch: 1500 | CrossEntropy-loss: 0.55771 | Correct: 1009/1280 | Difference: 1.0383998787102355

Epoch: 02 | Testing Accuracy: 404516/504607 (80.165%) | Historical Best: 80.165% 

Epoch: 03 | Batch: 000 | CrossEntropy-loss: 0.55989 | Correct: 1000/1280 | Difference: 1.0444738035365562
Epoch: 03 | Batch: 100 | CrossEntropy-loss: 0.53819 | Correct: 990/1280 | Difference: 1.051159170512885
Epoch: 03 | Batch: 200 | CrossEntropy-loss: 0.55762 | Correct: 1025/1280 | Difference: 1.0573580583489848
Epoch: 03 | Batch: 300 | CrossEntropy-loss: 0.50697 | Correct: 1031/1280 | Difference: 1.0587115171407837
Epoch: 03 | Batch: 400 | CrossEntropy-loss: 0.52693 | Correct: 1029/1280 | Difference: 1.0575749615673493
Epoch: 03 | Batch: 500 | CrossEntropy-loss: 0.49669 | Correct: 1043/1280 | Difference: 1.0583541232301603
Epoch: 03 | Batch: 600 | CrossEntropy-loss: 0.54060 | Correct: 1011/1280 | Difference: 1.058985385617851
Epoch: 03 | Batch: 700 | CrossEntropy-loss: 0.48254 | Correct: 1039/1280 | Difference: 1.0586691131535872
Epoch: 03 | Batch: 800 | CrossEntropy-loss: 0.55201 | Correct: 1018/1280 | Difference: 1.0595025502124766
Epoch: 03 | Batch: 900 | CrossEntropy-loss: 0.50069 | Correct: 1035/1280 | Difference: 1.0625482186555237
Epoch: 03 | Batch: 1000 | CrossEntropy-loss: 0.49555 | Correct: 1040/1280 | Difference: 1.0645258814198548
Epoch: 03 | Batch: 1100 | CrossEntropy-loss: 0.53857 | Correct: 1015/1280 | Difference: 1.063599770425
Epoch: 03 | Batch: 1200 | CrossEntropy-loss: 0.50130 | Correct: 1040/1280 | Difference: 1.063385637507066
Epoch: 03 | Batch: 1300 | CrossEntropy-loss: 0.47205 | Correct: 1063/1280 | Difference: 1.062217597130174
Epoch: 03 | Batch: 1400 | CrossEntropy-loss: 0.48781 | Correct: 1047/1280 | Difference: 1.0614487805679906
Epoch: 03 | Batch: 1500 | CrossEntropy-loss: 0.48665 | Correct: 1051/1280 | Difference: 1.061396790897268

Epoch: 03 | Testing Accuracy: 409804/504607 (81.213%) | Historical Best: 81.213% 

Epoch: 04 | Batch: 000 | CrossEntropy-loss: 0.49343 | Correct: 1050/1280 | Difference: 1.0614493219604588
Epoch: 04 | Batch: 100 | CrossEntropy-loss: 0.50074 | Correct: 1025/1280 | Difference: 1.0621167428894265
Epoch: 04 | Batch: 200 | CrossEntropy-loss: 0.48030 | Correct: 1048/1280 | Difference: 1.0632442345228816
Epoch: 04 | Batch: 300 | CrossEntropy-loss: 0.46044 | Correct: 1060/1280 | Difference: 1.0658091651215285
Epoch: 04 | Batch: 400 | CrossEntropy-loss: 0.49367 | Correct: 1022/1280 | Difference: 1.0692853572982253
Epoch: 04 | Batch: 500 | CrossEntropy-loss: 0.50433 | Correct: 1041/1280 | Difference: 1.0730098609163945
Epoch: 04 | Batch: 600 | CrossEntropy-loss: 0.46464 | Correct: 1062/1280 | Difference: 1.0760476015051734
Epoch: 04 | Batch: 700 | CrossEntropy-loss: 0.46948 | Correct: 1061/1280 | Difference: 1.079798492410594
Epoch: 04 | Batch: 800 | CrossEntropy-loss: 0.46021 | Correct: 1043/1280 | Difference: 1.0826103644361391
Epoch: 04 | Batch: 900 | CrossEntropy-loss: 0.47018 | Correct: 1045/1280 | Difference: 1.0866931176615189
Epoch: 04 | Batch: 1000 | CrossEntropy-loss: 0.48250 | Correct: 1030/1280 | Difference: 1.0903050834858685
Epoch: 04 | Batch: 1100 | CrossEntropy-loss: 0.44643 | Correct: 1053/1280 | Difference: 1.0933217351331148
Epoch: 04 | Batch: 1200 | CrossEntropy-loss: 0.47059 | Correct: 1063/1280 | Difference: 1.0960920006106807
Epoch: 04 | Batch: 1300 | CrossEntropy-loss: 0.49570 | Correct: 1037/1280 | Difference: 1.0982548463489716
Traceback (most recent call last):
  File "sdt_train.py", line 151, in <module>
    train_tree(tree)
  File "sdt_train.py", line 73, in train_tree
    prediction, output, penalty, weights = tree.forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 62, in forward
    _mu, _penalty, _alpha = self._forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 114, in _forward
    return mu, _penalty, torch.mean(torch.stack(half_alpha_list)).detach().cpu().numpy()   # mu contains the path probability for each leaf       
RuntimeError: expected a non-empty list of Tensors
