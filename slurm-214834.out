compute003
7
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Epoch: 01 | Batch: 000 | CrossEntropy-loss: 1.33801 | Correct: 502/1280 | Difference: 0.9122752361962396
Epoch: 01 | Batch: 100 | CrossEntropy-loss: 1.20698 | Correct: 683/1280 | Difference: 0.9463392518314522
Epoch: 01 | Batch: 200 | CrossEntropy-loss: 1.09087 | Correct: 688/1280 | Difference: 0.9830596806144798
Epoch: 01 | Batch: 300 | CrossEntropy-loss: 0.99550 | Correct: 694/1280 | Difference: 0.9906915198846579
Epoch: 01 | Batch: 400 | CrossEntropy-loss: 0.92043 | Correct: 762/1280 | Difference: 1.0035781387805696
Epoch: 01 | Batch: 500 | CrossEntropy-loss: 0.89322 | Correct: 885/1280 | Difference: 1.0148236637931403
Epoch: 01 | Batch: 600 | CrossEntropy-loss: 0.82634 | Correct: 978/1280 | Difference: 1.022201079567047
Epoch: 01 | Batch: 700 | CrossEntropy-loss: 0.77000 | Correct: 960/1280 | Difference: 1.036905598375594
Epoch: 01 | Batch: 800 | CrossEntropy-loss: 0.76858 | Correct: 948/1280 | Difference: 1.0533340489509857
Epoch: 01 | Batch: 900 | CrossEntropy-loss: 0.72733 | Correct: 959/1280 | Difference: 1.0697549352635032
Epoch: 01 | Batch: 1000 | CrossEntropy-loss: 0.69008 | Correct: 980/1280 | Difference: 1.0885662012580173
Epoch: 01 | Batch: 1100 | CrossEntropy-loss: 0.68064 | Correct: 974/1280 | Difference: 1.1053994747670937
Epoch: 01 | Batch: 1200 | CrossEntropy-loss: 0.68447 | Correct: 984/1280 | Difference: 1.1251697174271202
Epoch: 01 | Batch: 1300 | CrossEntropy-loss: 0.65783 | Correct: 1006/1280 | Difference: 1.1362359249540506
Epoch: 01 | Batch: 1400 | CrossEntropy-loss: 0.63411 | Correct: 1008/1280 | Difference: 1.1413781194702621
Epoch: 01 | Batch: 1500 | CrossEntropy-loss: 0.66364 | Correct: 988/1280 | Difference: 1.145366320415532

Epoch: 01 | Testing Accuracy: 391292/504607 (77.544%) | Historical Best: 77.544% 

Epoch: 02 | Batch: 000 | CrossEntropy-loss: 0.66688 | Correct: 986/1280 | Difference: 1.1503041669990115
Epoch: 02 | Batch: 100 | CrossEntropy-loss: 0.66032 | Correct: 978/1280 | Difference: 1.1589162387611112
Epoch: 02 | Batch: 200 | CrossEntropy-loss: 0.66594 | Correct: 972/1280 | Difference: 1.1714246554497219
Epoch: 02 | Batch: 300 | CrossEntropy-loss: 0.66226 | Correct: 993/1280 | Difference: 1.190021082381919
Epoch: 02 | Batch: 400 | CrossEntropy-loss: 0.63955 | Correct: 995/1280 | Difference: 1.215134786319733
Epoch: 02 | Batch: 500 | CrossEntropy-loss: 0.65231 | Correct: 991/1280 | Difference: 1.2397053987561903
Epoch: 02 | Batch: 600 | CrossEntropy-loss: 0.61919 | Correct: 1005/1280 | Difference: 1.2560651991718443
Epoch: 02 | Batch: 700 | CrossEntropy-loss: 0.62932 | Correct: 1010/1280 | Difference: 1.2667799253825867
Epoch: 02 | Batch: 800 | CrossEntropy-loss: 0.61425 | Correct: 995/1280 | Difference: 1.2796226511129587
Traceback (most recent call last):
  File "sdt_train.py", line 151, in <module>
    train_tree(tree)
  File "sdt_train.py", line 73, in train_tree
    prediction, output, penalty, weights = tree.forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 62, in forward
    _mu, _penalty, _alpha = self._forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 114, in _forward
    return mu, _penalty, torch.mean(torch.stack(half_alpha_list)).detach().cpu().numpy()   # mu contains the path probability for each leaf       
RuntimeError: expected a non-empty list of Tensors
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Epoch: 01 | Batch: 000 | CrossEntropy-loss: 1.41504 | Correct: 385/1280 | Difference: 0.9397412207064555
Epoch: 01 | Batch: 100 | CrossEntropy-loss: 1.31076 | Correct: 831/1280 | Difference: 0.9727156724365991
Epoch: 01 | Batch: 200 | CrossEntropy-loss: 1.20584 | Correct: 820/1280 | Difference: 1.004836513170269
Epoch: 01 | Batch: 300 | CrossEntropy-loss: 1.09524 | Correct: 938/1280 | Difference: 1.0230039930883295
Epoch: 01 | Batch: 400 | CrossEntropy-loss: 0.99507 | Correct: 870/1280 | Difference: 1.0230268423811542
Epoch: 01 | Batch: 500 | CrossEntropy-loss: 0.91248 | Correct: 867/1280 | Difference: 1.023803834165876
Epoch: 01 | Batch: 600 | CrossEntropy-loss: 0.81808 | Correct: 906/1280 | Difference: 1.0342301722234895
Epoch: 01 | Batch: 700 | CrossEntropy-loss: 0.84124 | Correct: 885/1280 | Difference: 1.0471988167442787
Epoch: 01 | Batch: 800 | CrossEntropy-loss: 0.78800 | Correct: 951/1280 | Difference: 1.046328634298214
Epoch: 01 | Batch: 900 | CrossEntropy-loss: 0.74499 | Correct: 965/1280 | Difference: 1.053562975355011
Epoch: 01 | Batch: 1000 | CrossEntropy-loss: 0.75057 | Correct: 944/1280 | Difference: 1.0588442896642878
Epoch: 01 | Batch: 1100 | CrossEntropy-loss: 0.75445 | Correct: 949/1280 | Difference: 1.0736967471482983
Epoch: 01 | Batch: 1200 | CrossEntropy-loss: 0.72990 | Correct: 939/1280 | Difference: 1.087099230421303
Epoch: 01 | Batch: 1300 | CrossEntropy-loss: 0.72371 | Correct: 948/1280 | Difference: 1.096050093381779
Epoch: 01 | Batch: 1400 | CrossEntropy-loss: 0.67428 | Correct: 978/1280 | Difference: 1.1102304117083932
Epoch: 01 | Batch: 1500 | CrossEntropy-loss: 0.65654 | Correct: 973/1280 | Difference: 1.1221584105359137

Epoch: 01 | Testing Accuracy: 373154/504607 (73.949%) | Historical Best: 73.949% 

Epoch: 02 | Batch: 000 | CrossEntropy-loss: 0.66923 | Correct: 967/1280 | Difference: 1.130733202838049
Epoch: 02 | Batch: 100 | CrossEntropy-loss: 0.68102 | Correct: 966/1280 | Difference: 1.1473537420141624
Epoch: 02 | Batch: 200 | CrossEntropy-loss: 0.63629 | Correct: 994/1280 | Difference: 1.1628783642066158
Epoch: 02 | Batch: 300 | CrossEntropy-loss: 0.67409 | Correct: 954/1280 | Difference: 1.1788023471682603
Epoch: 02 | Batch: 400 | CrossEntropy-loss: 0.64139 | Correct: 959/1280 | Difference: 1.193067296051331
Epoch: 02 | Batch: 500 | CrossEntropy-loss: 0.69719 | Correct: 932/1280 | Difference: 1.2077705237972833
Epoch: 02 | Batch: 600 | CrossEntropy-loss: 0.69042 | Correct: 927/1280 | Difference: 1.2307086685229927
Epoch: 02 | Batch: 700 | CrossEntropy-loss: 0.67071 | Correct: 947/1280 | Difference: 1.2532844917486203
Epoch: 02 | Batch: 800 | CrossEntropy-loss: 0.64533 | Correct: 966/1280 | Difference: 1.264021317664901
Epoch: 02 | Batch: 900 | CrossEntropy-loss: 0.64360 | Correct: 968/1280 | Difference: 1.2722781023722083
Epoch: 02 | Batch: 1000 | CrossEntropy-loss: 0.65343 | Correct: 952/1280 | Difference: 1.2751481227820132
Epoch: 02 | Batch: 1100 | CrossEntropy-loss: 0.67636 | Correct: 949/1280 | Difference: 1.2791425799453673
Epoch: 02 | Batch: 1200 | CrossEntropy-loss: 0.69154 | Correct: 937/1280 | Difference: 1.2829166520890358
Epoch: 02 | Batch: 1300 | CrossEntropy-loss: 0.65129 | Correct: 963/1280 | Difference: 1.2860405825084593
Epoch: 02 | Batch: 1400 | CrossEntropy-loss: 0.61847 | Correct: 969/1280 | Difference: 1.2895335393647112
Epoch: 02 | Batch: 1500 | CrossEntropy-loss: 0.61058 | Correct: 982/1280 | Difference: 1.2904512983152263

Epoch: 02 | Testing Accuracy: 378320/504607 (74.973%) | Historical Best: 74.973% 

Epoch: 03 | Batch: 000 | CrossEntropy-loss: 0.57847 | Correct: 1002/1280 | Difference: 1.2904286012516628
Epoch: 03 | Batch: 100 | CrossEntropy-loss: 0.64387 | Correct: 957/1280 | Difference: 1.29111175710989
Epoch: 03 | Batch: 200 | CrossEntropy-loss: 0.57060 | Correct: 988/1280 | Difference: 1.2910045951696447
Epoch: 03 | Batch: 300 | CrossEntropy-loss: 0.64324 | Correct: 956/1280 | Difference: 1.290177005504392
Epoch: 03 | Batch: 400 | CrossEntropy-loss: 0.61700 | Correct: 975/1280 | Difference: 1.2905928552889478
Epoch: 03 | Batch: 500 | CrossEntropy-loss: 0.61812 | Correct: 967/1280 | Difference: 1.289772632216962
Epoch: 03 | Batch: 600 | CrossEntropy-loss: 0.57668 | Correct: 1006/1280 | Difference: 1.2899996210500173
Epoch: 03 | Batch: 700 | CrossEntropy-loss: 0.58939 | Correct: 978/1280 | Difference: 1.2898149738891578
Epoch: 03 | Batch: 800 | CrossEntropy-loss: 0.62459 | Correct: 959/1280 | Difference: 1.2895826067690805
Epoch: 03 | Batch: 900 | CrossEntropy-loss: 0.62450 | Correct: 958/1280 | Difference: 1.2896900350034775
Epoch: 03 | Batch: 1000 | CrossEntropy-loss: 0.65027 | Correct: 938/1280 | Difference: 1.2896555206630538
Epoch: 03 | Batch: 1100 | CrossEntropy-loss: 0.59502 | Correct: 975/1280 | Difference: 1.2894970691765963
Epoch: 03 | Batch: 1200 | CrossEntropy-loss: 0.62097 | Correct: 963/1280 | Difference: 1.2892641763385504
Epoch: 03 | Batch: 1300 | CrossEntropy-loss: 0.59908 | Correct: 994/1280 | Difference: 1.2897432465317769
Epoch: 03 | Batch: 1400 | CrossEntropy-loss: 0.59763 | Correct: 976/1280 | Difference: 1.2902028442510016
Epoch: 03 | Batch: 1500 | CrossEntropy-loss: 0.62476 | Correct: 973/1280 | Difference: 1.2905044985499956

Epoch: 03 | Testing Accuracy: 383143/504607 (75.929%) | Historical Best: 75.929% 

Epoch: 04 | Batch: 000 | CrossEntropy-loss: 0.58978 | Correct: 990/1280 | Difference: 1.2907876718411948
Epoch: 04 | Batch: 100 | CrossEntropy-loss: 0.60176 | Correct: 975/1280 | Difference: 1.2908521289442445
Epoch: 04 | Batch: 200 | CrossEntropy-loss: 0.58280 | Correct: 988/1280 | Difference: 1.290569173070563
Epoch: 04 | Batch: 300 | CrossEntropy-loss: 0.59194 | Correct: 987/1280 | Difference: 1.290722335987695
Epoch: 04 | Batch: 400 | CrossEntropy-loss: 0.59327 | Correct: 988/1280 | Difference: 1.2906186686580887
Epoch: 04 | Batch: 500 | CrossEntropy-loss: 0.58321 | Correct: 1007/1280 | Difference: 1.2911219446884663
Epoch: 04 | Batch: 600 | CrossEntropy-loss: 0.55860 | Correct: 1017/1280 | Difference: 1.2909029586205454
Epoch: 04 | Batch: 700 | CrossEntropy-loss: 0.61217 | Correct: 991/1280 | Difference: 1.2910402899307132
Epoch: 04 | Batch: 800 | CrossEntropy-loss: 0.59021 | Correct: 988/1280 | Difference: 1.2909591253383816
Epoch: 04 | Batch: 900 | CrossEntropy-loss: 0.59379 | Correct: 993/1280 | Difference: 1.2907124299095531
Epoch: 04 | Batch: 1000 | CrossEntropy-loss: 0.56173 | Correct: 1034/1280 | Difference: 1.2905584071181364
Epoch: 04 | Batch: 1100 | CrossEntropy-loss: 0.57893 | Correct: 1013/1280 | Difference: 1.2909311288754257
Epoch: 04 | Batch: 1200 | CrossEntropy-loss: 0.61791 | Correct: 990/1280 | Difference: 1.2914791511382064
Epoch: 04 | Batch: 1300 | CrossEntropy-loss: 0.60305 | Correct: 988/1280 | Difference: 1.2911408753527687
Epoch: 04 | Batch: 1400 | CrossEntropy-loss: 0.56302 | Correct: 1034/1280 | Difference: 1.2911966245962245
Epoch: 04 | Batch: 1500 | CrossEntropy-loss: 0.65243 | Correct: 977/1280 | Difference: 1.2919828037915944

Epoch: 04 | Testing Accuracy: 395958/504607 (78.469%) | Historical Best: 78.469% 

Epoch: 05 | Batch: 000 | CrossEntropy-loss: 0.60069 | Correct: 1010/1280 | Difference: 1.2918795492992738
Epoch: 05 | Batch: 100 | CrossEntropy-loss: 0.61009 | Correct: 1015/1280 | Difference: 1.291745819752551
Epoch: 05 | Batch: 200 | CrossEntropy-loss: 0.57871 | Correct: 1025/1280 | Difference: 1.2921179632580377
Epoch: 05 | Batch: 300 | CrossEntropy-loss: 0.61354 | Correct: 991/1280 | Difference: 1.292684199598271
Epoch: 05 | Batch: 400 | CrossEntropy-loss: 0.58848 | Correct: 1009/1280 | Difference: 1.2925022972911302
Epoch: 05 | Batch: 500 | CrossEntropy-loss: 0.61306 | Correct: 992/1280 | Difference: 1.2927575115762506
Epoch: 05 | Batch: 600 | CrossEntropy-loss: 0.58435 | Correct: 1022/1280 | Difference: 1.2932963073705388
Epoch: 05 | Batch: 700 | CrossEntropy-loss: 0.59840 | Correct: 1007/1280 | Difference: 1.2934842294427245
Epoch: 05 | Batch: 800 | CrossEntropy-loss: 0.61562 | Correct: 994/1280 | Difference: 1.293818535189541
Epoch: 05 | Batch: 900 | CrossEntropy-loss: 0.57853 | Correct: 1010/1280 | Difference: 1.2937639504853986
Epoch: 05 | Batch: 1000 | CrossEntropy-loss: 0.57528 | Correct: 1020/1280 | Difference: 1.2938619667271496
Epoch: 05 | Batch: 1100 | CrossEntropy-loss: 0.56174 | Correct: 1029/1280 | Difference: 1.2939054034790967
Epoch: 05 | Batch: 1200 | CrossEntropy-loss: 0.60522 | Correct: 1009/1280 | Difference: 1.294381414165734
Epoch: 05 | Batch: 1300 | CrossEntropy-loss: 0.59207 | Correct: 1022/1280 | Difference: 1.2943040896188795
Epoch: 05 | Batch: 1400 | CrossEntropy-loss: 0.56634 | Correct: 1031/1280 | Difference: 1.2940400892144983
Epoch: 05 | Batch: 1500 | CrossEntropy-loss: 0.59617 | Correct: 1009/1280 | Difference: 1.294437470440895

Epoch: 05 | Testing Accuracy: 397666/504607 (78.807%) | Historical Best: 78.807% 

Epoch: 06 | Batch: 000 | CrossEntropy-loss: 0.57958 | Correct: 1007/1280 | Difference: 1.2944748364216492
Epoch: 06 | Batch: 100 | CrossEntropy-loss: 0.63889 | Correct: 984/1280 | Difference: 1.2947804892511532
Epoch: 06 | Batch: 200 | CrossEntropy-loss: 0.57888 | Correct: 1030/1280 | Difference: 1.2950677536115998
Epoch: 06 | Batch: 300 | CrossEntropy-loss: 0.60245 | Correct: 995/1280 | Difference: 1.295473542945505
Traceback (most recent call last):
  File "sdt_train.py", line 151, in <module>
    train_tree(tree)
  File "sdt_train.py", line 73, in train_tree
    prediction, output, penalty, weights = tree.forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 62, in forward
    _mu, _penalty, _alpha = self._forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 114, in _forward
    return mu, _penalty, torch.mean(torch.stack(half_alpha_list)).detach().cpu().numpy()   # mu contains the path probability for each leaf       
RuntimeError: expected a non-empty list of Tensors
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/zihan/anaconda3/envs/exp/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Epoch: 01 | Batch: 000 | CrossEntropy-loss: 1.40029 | Correct: 185/1280 | Difference: 0.9956655751019542
Epoch: 01 | Batch: 100 | CrossEntropy-loss: 1.29019 | Correct: 665/1280 | Difference: 1.0243612605353762
Epoch: 01 | Batch: 200 | CrossEntropy-loss: 1.18389 | Correct: 880/1280 | Difference: 1.0275995625227239
Epoch: 01 | Batch: 300 | CrossEntropy-loss: 1.06984 | Correct: 697/1280 | Difference: 1.0088700740685952
Epoch: 01 | Batch: 400 | CrossEntropy-loss: 0.97315 | Correct: 673/1280 | Difference: 0.9972883159361915
Epoch: 01 | Batch: 500 | CrossEntropy-loss: 0.87654 | Correct: 778/1280 | Difference: 0.9927537793257462
Epoch: 01 | Batch: 600 | CrossEntropy-loss: 0.85887 | Correct: 894/1280 | Difference: 0.9872643969521723
Epoch: 01 | Batch: 700 | CrossEntropy-loss: 0.80828 | Correct: 957/1280 | Difference: 0.9874963862354567
Epoch: 01 | Batch: 800 | CrossEntropy-loss: 0.77013 | Correct: 928/1280 | Difference: 0.9990563427331024
Epoch: 01 | Batch: 900 | CrossEntropy-loss: 0.71554 | Correct: 967/1280 | Difference: 1.0173204971493401
Epoch: 01 | Batch: 1000 | CrossEntropy-loss: 0.70316 | Correct: 950/1280 | Difference: 1.034505518535975
Epoch: 01 | Batch: 1100 | CrossEntropy-loss: 0.71414 | Correct: 945/1280 | Difference: 1.0568489009310378
Epoch: 01 | Batch: 1200 | CrossEntropy-loss: 0.65272 | Correct: 982/1280 | Difference: 1.0785176825345664
Epoch: 01 | Batch: 1300 | CrossEntropy-loss: 0.69613 | Correct: 936/1280 | Difference: 1.102999058254596
Epoch: 01 | Batch: 1400 | CrossEntropy-loss: 0.62846 | Correct: 968/1280 | Difference: 1.122462314388893
Epoch: 01 | Batch: 1500 | CrossEntropy-loss: 0.66954 | Correct: 927/1280 | Difference: 1.1366298230824783

Epoch: 01 | Testing Accuracy: 374329/504607 (74.182%) | Historical Best: 74.182% 

Epoch: 02 | Batch: 000 | CrossEntropy-loss: 0.66225 | Correct: 946/1280 | Difference: 1.1479217320196526
Epoch: 02 | Batch: 100 | CrossEntropy-loss: 0.64998 | Correct: 968/1280 | Difference: 1.158249434719115
Epoch: 02 | Batch: 200 | CrossEntropy-loss: 0.60349 | Correct: 991/1280 | Difference: 1.1694511892465107
Epoch: 02 | Batch: 300 | CrossEntropy-loss: 0.65177 | Correct: 931/1280 | Difference: 1.1806624814964113
Epoch: 02 | Batch: 400 | CrossEntropy-loss: 0.60857 | Correct: 970/1280 | Difference: 1.1895178398047603
Epoch: 02 | Batch: 500 | CrossEntropy-loss: 0.64331 | Correct: 937/1280 | Difference: 1.196088418553253
Epoch: 02 | Batch: 600 | CrossEntropy-loss: 0.61426 | Correct: 967/1280 | Difference: 1.2043099115703582
Epoch: 02 | Batch: 700 | CrossEntropy-loss: 0.63878 | Correct: 968/1280 | Difference: 1.2026189197923487
Epoch: 02 | Batch: 800 | CrossEntropy-loss: 0.60614 | Correct: 970/1280 | Difference: 1.2069328701159485
Epoch: 02 | Batch: 900 | CrossEntropy-loss: 0.61540 | Correct: 979/1280 | Difference: 1.216044489854606
Epoch: 02 | Batch: 1000 | CrossEntropy-loss: 0.63970 | Correct: 948/1280 | Difference: 1.2242749448025672
Epoch: 02 | Batch: 1100 | CrossEntropy-loss: 0.61072 | Correct: 982/1280 | Difference: 1.2331360225313497
Epoch: 02 | Batch: 1200 | CrossEntropy-loss: 0.57875 | Correct: 997/1280 | Difference: 1.240642933403446
Epoch: 02 | Batch: 1300 | CrossEntropy-loss: 0.64727 | Correct: 959/1280 | Difference: 1.2475854909388868
Epoch: 02 | Batch: 1400 | CrossEntropy-loss: 0.62255 | Correct: 971/1280 | Difference: 1.2567725221424932
Traceback (most recent call last):
  File "sdt_train.py", line 151, in <module>
    train_tree(tree)
  File "sdt_train.py", line 73, in train_tree
    prediction, output, penalty, weights = tree.forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 62, in forward
    _mu, _penalty, _alpha = self._forward(data)
  File "/home/zihan/Soft-Decision-Tree/SDT.py", line 114, in _forward
    return mu, _penalty, torch.mean(torch.stack(half_alpha_list)).detach().cpu().numpy()   # mu contains the path probability for each leaf       
RuntimeError: expected a non-empty list of Tensors
